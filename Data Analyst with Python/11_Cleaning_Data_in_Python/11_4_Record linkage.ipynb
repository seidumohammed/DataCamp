{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ced134-200e-4713-8246-bfd555613c53",
   "metadata": {},
   "source": [
    "# 4. Record linkage\n",
    "**Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07b092-7b54-409c-881d-dc3c5d8a10d0",
   "metadata": {},
   "source": [
    "## Comparing strings\n",
    "Welcome to the final chapter of this course, where we'll discover the world of record linkage. But before we get deep dive into record linkage, let's sharpen our understanding of string similarity and minimum edit distance.\n",
    "\n",
    "### Minimum edit distance\n",
    "Minimum edit distance is a systematic way to identify how close 2 strings are.\n",
    "\n",
    "For example, let's take a look at the following two words: *intention*, and *execution*.\n",
    "\n",
    "The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being inserting new characters, deleting them, substituting them, and transposing consecutive characters.\n",
    "\n",
    "To get from *intention* to *execution*, We first start off by deleting `I` from *intention*, and adding `C` between `E` and `N`. Our minimum edit distance so far is **2**, since these are two operations. Then we substitute the first `N` with `E`, `T` with `X`, and `N` with `U`, leading us to execution! With the minimum edit distance being **5**.\n",
    "\n",
    "The lower the edit distance, the closer two words are. For example, the two different typos of *reading*, '*reeding*' and '*redaing*', have a minimum edit distance of **1** between them and reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d43f9f-f0c3-4f19-bf4e-011b9342c974",
   "metadata": {},
   "source": [
    "### Minimum edit distance algorithm\n",
    "There's a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they're suited for and more, with a variety of packages to get each similarity.\n",
    "\n",
    "For this lesson, we'll be comparing strings using Levenshtein distance since it's the most general form of string matching by using the fuzzywuzzy package.\n",
    "\n",
    "Algorithm | Operations\n",
    ":---|:---\n",
    "Danerau-Levenshtein | insertion, substitution, deletion, transposition\n",
    "***Levenshtein*** | ***insertion, substitution, deletion***\n",
    "Hamming | substitution only\n",
    "Jaro distance | transpostion only\n",
    "... | ...\n",
    "\n",
    "**possible packages**: `nltk`, `fuzzywuzzy`, `textdistance`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaee7a9-5712-462d-8ff5-94bf56549a64",
   "metadata": {},
   "source": [
    "### Simple string comparison\n",
    "***Fuzzywuzzy*** is a simple to use package to perform string comparison. We first import fuzz from fuzzywuzzy, which allow us to compare between single strings. \n",
    "\n",
    "Here we use fuzz's WRatio function to compute the similarity between reading and its typo, inputting each string as an argument. For any comparison function using fuzzywuzzy, our output is a score from 0 to 100 with 0 being not similar at all, 100 being an exact match. \n",
    "\n",
    "```python\n",
    "# Compare between two strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading')\n",
    "```\n",
    "\n",
    "```\n",
    "86\n",
    "```\n",
    "\n",
    "Do not confuse this with the minimum edit distance score earlier, where a lower minimum edit distance means a closer match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc808879-2e2d-4712-8b33-e271250a50a1",
   "metadata": {},
   "source": [
    "### Partial strings and different orderings\n",
    "The WRatio function is highly robust against partial string comparison with different orderings. \n",
    "\n",
    "For example here we compare the strings Houston Rockets and Rockets, and still receive a high similarity score. \n",
    "```python\n",
    "# Partial string comparison\n",
    "fuzz.WRatio('Houston Rockets', 'Rockets')\n",
    "```\n",
    "```\n",
    "90\n",
    "```\n",
    "\n",
    "The same can be said for the strings Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets, where the team names are only partial and they are differently ordered.\n",
    "```python\n",
    "# Partial string comparison\n",
    "fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')\n",
    "```\n",
    "```\n",
    "86\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b3554-66e6-4d07-ac87-cea9247d2e1d",
   "metadata": {},
   "source": [
    "### Comparison with arrays\n",
    "We can also compare a string with an array of strings by using the extract function from the process module from fuzzy wuzzy. \n",
    "```python\n",
    "# Import precess\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Define string and array of possible matches\n",
    "string = 'Houston Rockets vs Los Angeles Lakers'\n",
    "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets',\n",
    "                    'Houston vs Los Angeles', 'Heat vs Bulls'])\n",
    "\n",
    "process.extract(string, choices, limit = 2)\n",
    "```\n",
    "```\n",
    "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]\n",
    "```\n",
    "Extract takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest. It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f79148-58e7-4ef6-bd98-a6394cb89a49",
   "metadata": {},
   "source": [
    "### Collapsing categories with string similarity\n",
    "**Chapter 2**\n",
    "\n",
    "Use `.replace()` to collapse `'eur` into `Europe`\n",
    "\n",
    "In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. \n",
    "\n",
    "*What if there are too many variations?*\n",
    "\n",
    "`'EU`, `'eur`, `'Europe'`, `'Europa'`, `'Erope'`, `'Evropa'` ...\n",
    "\n",
    "But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!\n",
    "\n",
    "Say we have DataFrame named survey containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. \n",
    "\n",
    "```python\n",
    "print(survey['state'].unique())\n",
    "```\n",
    "```\n",
    "id           state\n",
    "0       California\n",
    "1             Cali\n",
    "2       Calefornia\n",
    "3       Calefornie\n",
    "4       Californie\n",
    "5        Calfornia\n",
    "6       Calefernia\n",
    "7         New York\n",
    "8    New York City\n",
    "```\n",
    "\n",
    "The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we'll use string similarity. \n",
    "\n",
    "```python\n",
    "catefories\n",
    "```\n",
    "```\n",
    "   state\n",
    "0  California\n",
    "1  New York\n",
    "```\n",
    "\n",
    "We also have a category DataFrame containing the correct categories for each state. \n",
    "\n",
    "Let's collapse the incorrect categories with string matching!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75276c8c-44bc-4c0f-9d50-e2a97b33fd3c",
   "metadata": {},
   "source": [
    "### Collapsing all of the state\n",
    "\n",
    "```python\n",
    "# For each correct category\n",
    "for state in categories['state']:\n",
    "\t# Find potential matches in states with typoes\n",
    "\tmatches = process.extract(state, survey['state'], \n",
    "                              limit=survey.shape[0])\n",
    "\t# For each potential_match match\n",
    "\tfor potential_match in matches:\n",
    "\t\t# If high similarity score\n",
    "\t\tif potential_match[1] >= 80:\n",
    "\t\t\t# Replace type with correct category\n",
    "\t\t\tsurvey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
    "```\n",
    "\n",
    "We first create a for loop iterating over each correctly typed state in the categories DataFrame. \n",
    "\n",
    "For each state, we find its matches in the state column of the survey DataFrame, returning all possible matches by setting the limit argument of extract to the length of the survey DataFrame. \n",
    "\n",
    "Then we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement. \n",
    "\n",
    "Then for each of those returned strings, we replace it with the correct state using the loc method.\n",
    "\n",
    "### Record linkage\n",
    "Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc9651-1902-4168-9008-5f3557647ef3",
   "metadata": {},
   "source": [
    "### Minimum edit distance\n",
    "Minimum edit distance is the minimum number of steps needed to reach from String A to String B, with the operations available being:\n",
    "\n",
    "**Insertion** of a new character.\n",
    "**Deletion** of an existing character.\n",
    "**Substitution** of an existing character.\n",
    "**Transposition** of two existing consecutive characters.\n",
    "\n",
    "*What is the minimum edit distance from `'sign'` to `'sing'`, and which operation(s) gets you there?*\n",
    "\n",
    "1. ~2 by substituting `'g'` with `'n'` and `'n'` with `'g'`.~\n",
    "2. **1 by transposing `'g'` with `'n'`.**\n",
    "3. ~1 by substituting `'g'` with `'n'`.~\n",
    "4. ~2 by deleting `'g'` and inserting a new `'g'` at the end.~\n",
    "\n",
    "**Answer: 2.** Transposing the last two letters of `'sign'` is the easiest way to get to `'sing'`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7e74d-d99a-46aa-9a2f-6e080734940d",
   "metadata": {},
   "source": [
    "## The cutoff point\n",
    "The ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of the `restaurants` DataFrame has been collected from many sources, where the `cuisine_type` column is riddled with typos, and should contain only `italian`, `american` and `asian` cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the `fuzzywuzzy`'s `process.extract()` function by finding the similarity score of the most *distant* typo of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8afc50-e7a3-49a3-873e-150fe2dd23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "restaurants = pd.read_csv('restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef33df26-030b-4117-b303-0ba032d3c8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>addr</th>\n",
       "      <th>city</th>\n",
       "      <th>phone</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kokomo</td>\n",
       "      <td>6333 w. third st.</td>\n",
       "      <td>la</td>\n",
       "      <td>2139330773</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feenix</td>\n",
       "      <td>8358 sunset blvd. west</td>\n",
       "      <td>hollywood</td>\n",
       "      <td>2138486677</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>parkway</td>\n",
       "      <td>510 s. arroyo pkwy .</td>\n",
       "      <td>pasadena</td>\n",
       "      <td>8187951001</td>\n",
       "      <td>californian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>r-23</td>\n",
       "      <td>923 e. third st.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>2136877178</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gumbo</td>\n",
       "      <td>6333 w. third st.</td>\n",
       "      <td>la</td>\n",
       "      <td>2139330358</td>\n",
       "      <td>cajun/creole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     name                      addr         city       phone  \\\n",
       "0           0   kokomo         6333 w. third st.           la  2139330773   \n",
       "1           1   feenix   8358 sunset blvd. west     hollywood  2138486677   \n",
       "2           2  parkway      510 s. arroyo pkwy .     pasadena  8187951001   \n",
       "3           3     r-23          923 e. third st.  los angeles  2136877178   \n",
       "4           4    gumbo         6333 w. third st.           la  2139330358   \n",
       "\n",
       "           type  \n",
       "0      american  \n",
       "1      american  \n",
       "2   californian  \n",
       "3      japanese  \n",
       "4  cajun/creole  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31842ac-8a8b-4fd3-9ac5-4bf19b5a6294",
   "metadata": {},
   "source": [
    "- Import `process` from `fuzzywuzzy`.\n",
    "- Store the unique `cuisine_types` into `unique_types`.\n",
    "- Calculate the similarity of `'asian'`, `'american'`, and `'italian'` to all possible `cuisine_types` using `process.extract()`, while returning all possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0fbb72-64eb-46fe-9181-e7061d8a6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5449a7e6-e4b8-49ff-8c98-703198290438",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('indonesian', 72), ('italian', 67), ('russian', 67), ('american', 62), ('californian', 54), ('japanese', 54), ('mexican/tex-mex', 54), ('american ( new )', 54), ('mexican', 50), ('cajun/creole', 36), ('middle eastern', 36), ('vietnamese', 36), ('pacific new wave', 36), ('fast food', 36), ('chicken', 33), ('hamburgers', 27), ('hot dogs', 26), ('coffeebar', 26), ('continental', 26), ('steakhouses', 25), ('southern/soul', 22), ('delis', 20), ('eclectic', 20), ('pizza', 20), ('health food', 19), ('diners', 18), ('coffee shops', 18), ('noodle shops', 18), ('french ( new )', 18), ('desserts', 18), ('seafood', 17), ('chinese', 17)] \n",
      "\n",
      "[('american', 100), ('american ( new )', 90), ('mexican', 80), ('mexican/tex-mex', 68), ('asian', 62), ('italian', 53), ('russian', 53), ('middle eastern', 51), ('pacific new wave', 45), ('hamburgers', 44), ('indonesian', 44), ('chicken', 40), ('southern/soul', 39), ('japanese', 38), ('eclectic', 38), ('delis', 36), ('pizza', 36), ('cajun/creole', 34), ('french ( new )', 34), ('vietnamese', 33), ('californian', 32), ('diners', 29), ('desserts', 25), ('coffeebar', 24), ('steakhouses', 21), ('seafood', 13), ('chinese', 13), ('fast food', 12), ('coffee shops', 11), ('noodle shops', 11), ('health food', 11), ('continental', 11), ('hot dogs', 0)] \n",
      "\n",
      "[('italian', 100), ('asian', 67), ('californian', 56), ('continental', 51), ('indonesian', 47), ('russian', 43), ('mexican', 43), ('american', 40), ('japanese', 40), ('mexican/tex-mex', 39), ('american ( new )', 39), ('pacific new wave', 39), ('vietnamese', 35), ('delis', 33), ('pizza', 33), ('diners', 31), ('middle eastern', 30), ('chicken', 29), ('chinese', 29), ('health food', 27), ('southern/soul', 27), ('cajun/creole', 26), ('steakhouses', 26), ('seafood', 14), ('hot dogs', 13), ('noodle shops', 13), ('eclectic', 13), ('french ( new )', 13), ('desserts', 13), ('hamburgers', 12), ('fast food', 12), ('coffeebar', 12), ('coffee shops', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit=len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit=len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28997b-b850-41b8-b2fb-ae34cade7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit=len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit=len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201b5a5-ed92-484b-8660-86bfb4482cf1",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "[('asian', 100), ('asiane', 91), ('asiann', 91), ('asiian', 91), ('asiaan', 91), ('asianne', 83), ('asiat', 80), ('italiann', 72), ('italiano', 72), ('italianne', 72), ('italian', 67), ('amurican', 62), ('american', 62), ('italiaan', 62), ('italiian', 62), ('itallian', 62), ('americann', 57), ('americano', 57), ('ameerican', 57), ('aamerican', 57), ('ameriican', 57), ('amerrican', 57), ('ammericann', 54), ('ameerrican', 54), ('ammereican', 54), ('america', 50), ('merican', 50), ('murican', 50), ('italien', 50), ('americen', 46), ('americin', 46), ('amerycan', 46), ('itali', 40)]\n",
    "\n",
    "[('american', 100), ('americann', 94), ('americano', 94), ('ameerican', 94), ('aamerican', 94), ('ameriican', 94), ('amerrican', 94), ('america', 93), ('merican', 93), ('ammericann', 89), ('ameerrican', 89), ('ammereican', 89), ('amurican', 88), ('americen', 88), ('americin', 88), ('amerycan', 88), ('murican', 80), ('asian', 62), ('asiane', 57), ('asiann', 57), ('asiian', 57), ('asiaan', 57), ('italian', 53), ('asianne', 53), ('italiann', 50), ('italiano', 50), ('italiaan', 50), ('italiian', 50), ('itallian', 50), ('italianne', 47), ('asiat', 46), ('itali', 40), ('italien', 40)]\n",
    "\n",
    "[('italian', 100), ('italiann', 93), ('italiano', 93), ('italiaan', 93), ('italiian', 93), ('itallian', 93), ('italianne', 88), ('italien', 86), ('itali', 83), ('asian', 67), ('asiane', 62), ('asiann', 62), ('asiian', 62), ('asiaan', 62), ('asianne', 57), ('amurican', 53), ('american', 53), ('americann', 50), ('asiat', 50), ('americano', 50), ('ameerican', 50), ('aamerican', 50), ('ameriican', 50), ('amerrican', 50), ('ammericann', 47), ('ameerrican', 47), ('ammereican', 47), ('america', 43), ('merican', 43), ('murican', 43), ('americen', 40), ('americin', 40), ('amerycan', 40)]\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e53ad-2ce7-49bd-9a61-27d49ad3b9ad",
   "metadata": {},
   "source": [
    "### Question\n",
    "Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?\n",
    "\n",
    "1. **80**\n",
    "2. ~70~ - (*Not quite, `'asian'` and `'italiann'` have a similarity score of ***72***, meaning `'italiann'` would be converted to `'asian'`.*)\n",
    "3. ~60~ - (*That's too low, and you risk converting a lot more categories than you should.*)\n",
    "\n",
    "**Answer: 1.** **80** is that sweet spot where you convert all incorrect typos without remapping incorrect categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73c5de-a2e8-4683-bb92-5c44d69db64a",
   "metadata": {},
   "source": [
    "## Remapping categories II\n",
    "In the last exercise, you determined that the distance cutoff point for remapping typos of `'american'`, `'asian'`, and `'italian'` cuisine types stored in the `cuisine_type` column should be **80**.\n",
    "\n",
    "In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using `fuzywuzzy.process`'s `extract()` function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using `process.extract()`, the output is a list of tuples where each is formatted like:\n",
    "```\n",
    "(closest match, similarity score, index of match)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41100a5a-9cef-4982-87b4-e87032148c83",
   "metadata": {},
   "source": [
    "- Return all of the unique values in the `cuisine_type` column of `restaurants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a520ee-49dc-49d6-a095-706d587bf695",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'californian' 'japanese' 'cajun/creole' 'hot dogs' 'diners'\n",
      " 'delis' 'hamburgers' 'seafood' 'italian' 'coffee shops' 'russian'\n",
      " 'steakhouses' 'mexican/tex-mex' 'noodle shops' 'mexican' 'middle eastern'\n",
      " 'asian' 'vietnamese' 'health food' 'american ( new )' 'pacific new wave'\n",
      " 'indonesian' 'eclectic' 'chicken' 'fast food' 'southern/soul' 'coffeebar'\n",
      " 'continental' 'french ( new )' 'desserts' 'chinese' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a202d-8f08-4528-91eb-8fb5ef66dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab3546-da3e-41c8-a732-50611ee8b975",
   "metadata": {},
   "source": [
    "```\n",
    "['america' 'merican' 'amurican' 'americen' 'americann' 'asiane' 'itali' 'asiann' 'murican' 'italien' 'italian' 'asiat' 'american' 'americano' 'italiann' 'ameerican' 'asianne' 'italiano' 'americin' 'ammericann' 'amerycan' 'aamerican' 'ameriican' 'italiaan' 'asiian' 'asiaan' 'amerrican' 'ameerrican' 'ammereican' 'asian' 'italianne' 'italiian' 'itallian']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cd22c-f025-4b01-a67c-eb9a95780419",
   "metadata": {},
   "source": [
    "Looks like you will need to use some string matching to correct these misspellings.\n",
    "\n",
    "- As a first step, create a list of all possible `matches`, comparing `'italian'` with the restaurant types listed in the `cuisine_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388fd352-b1ee-4311-894f-5036c17176fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italian', 100, 14), ('italian', 100, 21), ('italian', 100, 47), ('italian', 100, 57), ('italian', 100, 73)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['type'], limit=len(restaurants.type))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99ac45-1f7f-410f-b460-6efba565a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2ae48-c525-4181-ae97-ab8c347281a7",
   "metadata": {},
   "source": [
    "```\n",
    "[('italian', 100, 11), ('italian', 100, 25), ('italian', 100, 41), ('italian', 100, 47), ('italian', 100, 49)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e868ba-710c-4854-a884-3fb9627e65c6",
   "metadata": {},
   "source": [
    "Now you're getting somewhere! Now you can iterate through `matches` to reassign similar entries.\n",
    "\n",
    "- Within the `for` loop, use an `if` statement to check whether the similarity score in each `match` is greater than or equal to 80.\n",
    "- If it is, use `.loc` to select rows where `cuisine_type` in `restaurants` is equal to the current match (which is the first element of `match`), and reassign them to be `'italian'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3f8587-51a3-4c20-8e85-ddcc4359d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['type'] == match[0]] = 'italian'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c396f-61f6-4ee0-b678-0de418d0d78e",
   "metadata": {},
   "source": [
    "Finally, you'll adapt your code to work with every restaurant type in `categories`.\n",
    "\n",
    "- Using the variable `cuisine` to iterate through `categories`, embed your code from the previous step in an outer `for` loop.\n",
    "- Inspect the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "929ad26c-3617-48dc-bb2e-365c9ad2101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['italian', 'asian', 'american']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3382945-ea14-4b9a-9bcc-333ea1d0f054",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'californian' 'japanese' 'cajun/creole' 'hot dogs' 'diners'\n",
      " 'delis' 'hamburgers' 'seafood' 'italian' 'coffee shops' 'russian'\n",
      " 'steakhouses' 'mexican/tex-mex' 'noodle shops' 'middle eastern' 'asian'\n",
      " 'vietnamese' 'health food' 'pacific new wave' 'indonesian' 'eclectic'\n",
      " 'chicken' 'fast food' 'southern/soul' 'coffeebar' 'continental'\n",
      " 'french ( new )' 'desserts' 'chinese' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['type'], limit=len(restaurants.type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "        # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "        restaurants.loc[restaurants['type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result\n",
    "print(restaurants['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7f615-6a38-4a40-807f-883c28a7983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "        # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "        restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4a7aa-e358-40ba-a2c0-8cc06d4212d1",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "['american' 'asian' 'italian']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e471396-8260-4f74-b4b6-fd4784baeb88",
   "metadata": {},
   "source": [
    "*All your cuisine types are properly mapped. Now you'll build on string similarity, by jumping into record linkage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ba8c3-0a31-4507-9cfc-0b8b2448ed31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c33dce-6afb-4966-ba74-5cf0a81550cc",
   "metadata": {},
   "source": [
    "## Generating pairs\n",
    "### Record linkage\n",
    "Record linkage is the act of linking data from different sources regarding the same entity. Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. All of these steps can be achieved with the `recordlinkage` package.\n",
    "\n",
    "### DataFrames\n",
    "There are two DataFrames, census_A, and census_B, containing data on individuals throughout the states. \n",
    "```python\n",
    "census_A\n",
    "```\n",
    "```\n",
    "              given_name  surname  date_of_birth         suburb  state  address_1\n",
    "rec_id\n",
    "rec-1070-org    michaela  neumann       19151111  winston hills    cal  stanley street\n",
    "rec-1016-org    courtney  painter       19161214      richlands    txs  pinkerton circuit\n",
    "```\n",
    "```python\n",
    "census_B\n",
    "```\n",
    "```\n",
    "                given_name  surname  date_of_birth      suburb  state  address_1\n",
    "rec_id\n",
    "rec-561-dup-0        elton      NaN       19651013  windermere     ny  light setreet\n",
    "rec-2642-dup-0    mitchell    maxon       19390212  north ryde    cal  edkins street\n",
    "```\n",
    "\n",
    "We want to merge them while avoiding duplication using record linkage, since they are collected manually and are prone to typos, there are no consistent IDs between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36427cc-4d71-47db-bd4f-6672a173112c",
   "metadata": {},
   "source": [
    "### Generating pairs\n",
    "We first want to generate pairs between both DataFrames. Ideally, we want to generate all possible pairs between our DataFrames, but what if we had big DataFrames and ended up having to generate millions if not billions of pairs? It wouldn't prove scalable and could seriously hamper development time.\n",
    "\n",
    "### Blocking\n",
    "This is where we apply what we call blocking, which creates pairs based on a matching column, which is in this case, the state column, reducing the number of possible pairs.\n",
    "\n",
    "### Generating pairs\n",
    "To do this, we first start off by importing recordlinkage. We then use the recordlinkage dot Index function, to create an indexing object. This essentially is an object we can use to generate pairs from our DataFrames. To generate pairs blocked on state, we use the block method, inputting the state column as input. \n",
    "```python\n",
    "# Import recordlinkage\n",
    "import recordlinkage\n",
    "\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Generate pairs blocked on state\n",
    "indexer.block('state')\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "```\n",
    "\n",
    "Once the indexer object has been initialized, we generate our pairs using the dot index method, which takes in the two dataframes.\n",
    "\n",
    "```python\n",
    "print(pairs)\n",
    "```\n",
    "```\n",
    "MultiIndex(levels=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org', 'rec-1070-org', 'rec-1075-org', 'rec-1080-org', 'rec-110-org', 'rec-1146-org', 'rec-1157-org', 'rec-1165-org', 'rec-1185-org', 'rec-1234-org', 'rec-1271-org', 'rec-1280-org', ......\n",
    "66, 14, 13, 18, 34, 39, 0, 16, 80, 50, 20, 69, 28, 25, 49, 77, 51, 85, 52, 63, 74, 61, 83, 91, 22, 26, 55, 84, 11, 81, 97, 56, 27, 48, 2, 64, 5, 17, 29, 60, 72, 47, 92, 12, 95, 15, 19, 57, 37, 70, 94]], names=['rec_id_1', 'rec_id_2'])\n",
    "```\n",
    "\n",
    "The resulting object, is a pandas multi index object containing pairs of row indices from both DataFrames, which is a fancy way to say it is an array containing possible pairs of indices that makes it much easier to subset DataFrames on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d4732-5e82-45a9-866c-13f2bc21f115",
   "metadata": {},
   "source": [
    "### Comparing the DataFrames\n",
    "Since we've already generated our pairs, it's time to find potential matches. \n",
    "\n",
    "```python\n",
    "# Generate the pairs\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "# Create a Compare object\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches for pairs of date_of_birth and state\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_cl.exacy('state', 'state', label='state')\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity\n",
    "compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n",
    "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
    "\n",
    "# Find matches\n",
    "potential_matches = compare_cl.compute(pairs, census_A, census_B)\n",
    "```\n",
    "\n",
    "We first start by creating a comparison object using the recordlinkage dot compare function. This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs. Let's say there are columns for which we want exact matches between the pairs. \n",
    "\n",
    "To do that, we use the exact method. It takes in the column name in question for each DataFrame, which is in this case date_of_birth and state, and a label argument which lets us set the column name in the resulting DataFrame. \n",
    "\n",
    "Now in order to compute string similarities between pairs of rows for columns that have fuzzy values, we use the dot string method, which also takes in the column names in question, the similarity cutoff point in the threshold argument, which takes in a value between 0 and 1, which we here set to 0.85. \n",
    "\n",
    "Finally to compute the matches, we use the compute function, which takes in the possible pairs, and the two DataFrames in question. \n",
    "\n",
    "Note that you need to always have the same order of DataFrames when inserting them as arguments when generating pairs, comparing between columns, and computing comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd90495-bade-40c4-8144-09724f85d0e2",
   "metadata": {},
   "source": [
    "### Finding matching pairs\n",
    "The output is a multi index DataFrame, where the first index is the row index from the first DataFrame, or census A, and the second index is a list of all row indices in census B. The columns are the columns being compared, with values being 1 for a match, and 0 for not a match.\n",
    "\n",
    "```python\n",
    "print(potential_matches)\n",
    "```\n",
    "```\n",
    "                             date_of_birth  state  surname  address_1\n",
    "rec_id_1     rec_id_2       \n",
    "rec-1070-org rec-561-dup-0               0      1      0.0        0.0\n",
    "             rec-2642-dup-0              0      1      0.0        0.0\n",
    "             rec-608-dup-0               0      1      0.0        0.0\n",
    "...\n",
    "rec-1631-org rec-4070-dup-0              0      1      0.0        0.0\n",
    "             rec-4862-dup-0              0      1      0.0        0.0\n",
    "             rec-629-dup-0               0      1      0.0        0.0\n",
    "...\n",
    "```\n",
    "\n",
    "To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. Which in this case higher or equal to 2. But we'll dig deeper into these matches and see how to use them to link our census DataFrames in the next lesson.\n",
    "```python\n",
    "potential_matches[pontential_matches.sum(axis = 1) => 2]\n",
    "```\n",
    "```\n",
    "                             date_of_birth  state  surname  address_1\n",
    "rec_id_1     rec_id_2       \n",
    "rec-4878-org rec-4878-dup-0              1      1      1.0        0.0\n",
    "rec-417-org  rec-2867-dup-0              0      1      0.0        1.0\n",
    "rec-3967-org rec-394-dup-0               0      1      1.0        0.0\n",
    "rec-1373-org rec-4051-dup-0              0      1      1.0        0.0\n",
    "             rec-802-dup-0               0      1      1.0        0.0\n",
    "rec-3540-org rec-470-dup-0               0      1      1.0        0.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5991b0-776b-46a9-9754-a17009d52161",
   "metadata": {},
   "source": [
    "## To link or not to link?\n",
    "Similar to joins, record linkage is the act of linking data from different sources regarding the same entity. But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely upon when linking data sources such as a unique identifier.\n",
    "\n",
    "In this exercise, you will classify each card whether it is a traditional join problem, or a record linkage one.\n",
    "\n",
    "- Classify each card into a problem that requires record linkage or regular joins.\n",
    "\n",
    "Record linkage | Regular joins\n",
    ":---|:---\n",
    "Two customer DataFrames containing names and address, one with a unique identifier per customer, one without. | Two basketball DataFrames with a common unique identifier per game.\n",
    "Using an `address` column to join two DataFrames, with the address in each DataFrame being formatted slightly differently. | Consolidationg two DataFrames containing details on the courses, with each course having its own unique indentifier.\n",
    "Merging two basketball DataFrames, with columns `team_A`, `team_B`, and `time` and differently formatted team names between each DataFrame. |\n",
    "\n",
    "*Don't make things more complicated than they need to be: record linkage is a powerful tool, but it's more complex than using a traditional join.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78edc5-ccc6-48b4-9bfa-276c42b59764",
   "metadata": {},
   "source": [
    "## Pairs of restaurants\n",
    "In the last lesson, you cleaned the `restaurants` dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named `restaurants_new` with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "\n",
    "You've already cleaned the `cuisine_type` and `city` columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names that require record linkage instead of joins with `restaurants`.\n",
    "\n",
    "In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between `restaurants` and `restaurants_new`.\n",
    "\n",
    "- Instantiate an indexing object by using the `Index()` function from `recordlinkage`.\n",
    "- Block your pairing on `cuisine_type` by using `indexer`'s' `.block()` method.\n",
    "- Generate pairs by indexing `restaurants` and `restaurants_new` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b687fe5-1b2b-4061-9cf2-13007cb97b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "906031c4-ef98-4d38-8ceb-03f05b21ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_new = pd.read_csv('restaurants_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f030f1d-65a8-468d-9051-09840a2b64b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>addr</th>\n",
       "      <th>city</th>\n",
       "      <th>phone</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>435 s. la cienega blv .</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>3102461501</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>art's delicatessen</td>\n",
       "      <td>12224 ventura blvd.</td>\n",
       "      <td>studio city</td>\n",
       "      <td>8187621221</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>campanile</td>\n",
       "      <td>624 s. la brea ave.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>2139381447</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fenix</td>\n",
       "      <td>8358 sunset blvd. west</td>\n",
       "      <td>hollywood</td>\n",
       "      <td>2138486677</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>grill on the alley</td>\n",
       "      <td>9560 dayton way</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>3102760615</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       name                       addr  \\\n",
       "0           0  arnie morton's of chicago   435 s. la cienega blv .    \n",
       "1           1         art's delicatessen       12224 ventura blvd.    \n",
       "2           2                  campanile       624 s. la brea ave.    \n",
       "3           3                      fenix    8358 sunset blvd. west    \n",
       "4           4         grill on the alley           9560 dayton way    \n",
       "\n",
       "          city       phone      type  \n",
       "0  los angeles  3102461501  american  \n",
       "1  studio city  8187621221  american  \n",
       "2  los angeles  2139381447  american  \n",
       "3    hollywood  2138486677  american  \n",
       "4  los angeles  3102760615  american  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c410322c-a039-4c2a-a6b5-a9fdfdaeee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9b496-e0e9-4099-b8be-effa6bb99a20",
   "metadata": {},
   "source": [
    "### Question\n",
    "Now that you've generated your pairs, you've achieved the first step of record linkage. What are the steps remaining to link both restaurants DataFrames, and in what order?\n",
    "\n",
    "1. **Compare between columns, score the comparison, then link the DataFrames.**\n",
    "2. ~Clean the data, compare between columns, link the DataFrames, then score the comparison.~\n",
    "3. ~Clean the data, compare between columns, score the comparison, then link the DataFrames.~\n",
    "\n",
    "**Answer: 1.** Cleaning data precedes the pair generation phase, and linking DataFrames is the final step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f3127-6755-406f-9e49-053dcc4a3c90",
   "metadata": {},
   "source": [
    "## Similar restaurants\n",
    "In the last exercise, you generated pairs between `restaurants` and `restaurants_new` in an effort to cleanly merge both DataFrames using record linkage.\n",
    "\n",
    "When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "Now that your pairs have been generated and stored in `pairs`, you will find exact matches in the `city` and `cuisine_type` columns between each pair, and similar strings for each pair in the `rest_name` column.\n",
    "\n",
    "- Instantiate a comparison object using the `recordlinkage.Compare()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4bcd25b-de16-4764-9f29-8c0354c2d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9de1a3-5fab-4d1e-afcf-d7558445d1c7",
   "metadata": {},
   "source": [
    "- Use the appropriate `comp_cl` method to find exact matches between the `city` and `cuisine_type` columns of both DataFrames.\n",
    "- Use the appropriate `comp_cl` method to find similar strings with a `0.8` similarity threshold in the `rest_name` column of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb54cca9-8bea-4b6f-8b4d-4937f3e988e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('type', 'type', label='type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold=0.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8faa08a-d239-4506-8f53-a784f1cfd966",
   "metadata": {},
   "source": [
    "- Compute the comparison of the pairs by using the `.compute()` method of `comp_cl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330ca19-e48e-4f41-8092-a3aeda3e9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be1812-f533-4979-8422-2bc57654ba04",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "            city  cuisine_type  name\n",
    "    0   0      0             1   0.0\n",
    "        1      0             1   0.0\n",
    "        7      0             1   0.0\n",
    "        12     0             1   0.0\n",
    "        13     0             1   0.0\n",
    "    ...      ...           ...   ...\n",
    "    40  18     0             1   0.0\n",
    "    281 18     0             1   0.0\n",
    "    288 18     0             1   0.0\n",
    "    302 18     0             1   0.0\n",
    "    308 18     0             1   0.0\n",
    "    \n",
    "    [3631 rows x 3 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d66604-2c9b-4ddd-9d53-54a246733c7a",
   "metadata": {},
   "source": [
    "### Question\n",
    "Print out `potential_matches`, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with\n",
    "```python\n",
    "potential_matches[potential_matches.sum(axis = 1) >= n]\n",
    "```\n",
    "\n",
    "Where `n` is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of `n` be?\n",
    "\n",
    "1. **3 because I need to have matches in all my columns.**\n",
    "2. ~2 because matching on any of the 2 columns or more is enough to find potential duplicates.~ (If `n` is set to 2, then you will get duplicates for all restaurants with the same cuisine type in the same city.)\n",
    "3. ~1 because matching on just 1 column like the restaurant name is enough to find potential duplicates.~ (What if you had restaurants with the same name in different cities?)\n",
    "\n",
    "**Answer: 1.** For this example, tightening your selection criteria will ensure good duplicate finds. In the next lesson, you're gonna build on what you learned to link these two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038bc95-7910-4c51-a2ce-b91e30951f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[potential_matches.sum(axis = 1) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438ca3f-c61c-46d6-a4d9-51607b5dac84",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "       city  cuisine_type  name\n",
    "0  40     1             1   1.0\n",
    "1  28     1             1   1.0\n",
    "2  74     1             1   1.0\n",
    "3  1      1             1   1.0\n",
    "4  53     1             1   1.0\n",
    "8  43     1             1   1.0\n",
    "9  50     1             1   1.0\n",
    "13 7      1             1   1.0\n",
    "14 67     1             1   1.0\n",
    "17 12     1             1   1.0\n",
    "20 20     1             1   1.0\n",
    "21 27     1             1   1.0\n",
    "5  65     1             1   1.0\n",
    "7  79     1             1   1.0\n",
    "12 26     1             1   1.0\n",
    "18 71     1             1   1.0\n",
    "6  73     1             1   1.0\n",
    "10 75     1             1   1.0\n",
    "11 21     1             1   1.0\n",
    "16 57     1             1   1.0\n",
    "19 47     1             1   1.0\n",
    "15 55     1             1   1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f018b-b269-4ede-b909-a4b9c408a0dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817debf2-ebc2-4cfb-838d-ee639b05e16a",
   "metadata": {},
   "source": [
    "## Linking DataFrames\n",
    "### Potential matches\n",
    "Let's look closely at the potential matches. It is a multi-index DataFrame, where we have two index columns, record id 1, and record id 2. \n",
    "```python\n",
    "potential_matches\n",
    "```\n",
    "```\n",
    "                             date_of_birth  state  surname  address_1\n",
    "rec_id_1     rec_id_2       \n",
    "rec-1070-org rec-561-dup-0               0      1      0.0        0.0\n",
    "             rec-2642-dup-0              0      1      0.0        0.0\n",
    "             rec-608-dup-0               0      1      0.0        0.0\n",
    "...                                    ...    ...      ...        ...\n",
    "rec-1631-org rec-1697-dup-0              0      1      0.0        0.0\n",
    "             rec-4404-dup-0              0      1      0.0        0.0\n",
    "             rec-3780-dup-0              0      1      0.0        0.0\n",
    "...                                    ...    ...      ...        ...\n",
    "```\n",
    "The first index column, stores indices from census A. The second index column, stores all possible indices from census_B, for each row index of census_A. The columns of our potential matches are the columns we chose to link both DataFrames on, where the value is 1 for a match, and 0 otherwise.\n",
    "\n",
    "### Probable matches\n",
    "The first step in linking DataFrames, is to isolate the potentially matching pairs to the ones we're pretty sure of. We saw how to do this in the previous lesson, by subsetting the rows where the row sum is above a certain number of columns, in this case 3. \n",
    "```python\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "```\n",
    "```\n",
    "                             date_of_birth  state  surname  address_1\n",
    "rec_id_1     rec_id_2       \n",
    "rec-2404-org rec-2404-dup-0              1      1      1.0        1.0\n",
    "rec-4178-org rec-4178-dup-0              1      1      1.0        1.0\n",
    "rec-1054-org rec-1054-dup-0              1      1      1.0        1.0\n",
    "...                                    ...    ...      ...        ...\n",
    "rec-1234-org rec-1234-dup-0              1      1      1.0        1.0\n",
    "rec-1271-org rec-1271-dup-0              1      1      1.0        1.0\n",
    "```\n",
    "The output is row indices between census A and census B that are most likely duplicates. The next step is to extract the one of the index columns, and subsetting its associated DataFrame to filter for duplicates.\n",
    "\n",
    "Here we choose the second index column, which represents row indices of `census B`. We want to extract those indices, and subset `census_B` on them to remove duplicates with `census_A` before appending them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3da5c9-58e5-46d2-86c5-101ed1583889",
   "metadata": {},
   "source": [
    "### Get the indices\n",
    "```python\n",
    "matches.index\n",
    "```\n",
    "```\n",
    "MultiIndex(levels=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org', 'rec-1070-org', 'rec-1075-org', 'rec-1080-org', 'rec-110-org',  ...\n",
    "```\n",
    "We can access a DataFrame's index using the index attribute. Since this is a multi index DataFrame, it returns a multi index object containing pairs of row indices from `census_A` and `census_B` respectively. \n",
    "```python\n",
    "# Get indices from census_B only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "print(census_B_index)\n",
    "```\n",
    "```\n",
    "MultiIndex(levels=[['rec-2404-dup-0', 'rec-4178-dup-0', 'rec-1054-dup-0', 'rec-4663-dup-0', 'rec-485-dup-0', 'rec-2950-dup-0', 'rec-1234-dup-0', ... 'rec-299-oduprg-0'])\n",
    "```\n",
    "We want to extract all `census_B` indices, so we chain it with the get_level_values method, which takes in which column index we want to extract its values. We can either input the index column's name, or its order, which is in this case 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849428c2-3332-458f-a18a-3bf85d8dfdf3",
   "metadata": {},
   "source": [
    "### Linking DataFrames\n",
    "```python\n",
    "# Finding duplicates in census_B\n",
    "census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
    "\n",
    "# Finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "```\n",
    "To find the duplicates in `census B`, we simply subset on all indices of `census_B`, with the ones found through record linkage. You can choose to examine them further for similarity with their duplicates in `census_A`, but if you're sure of your analysis, you can go ahead and find the non duplicates by repeating the exact same line of code, except by adding a tilde at the beginning of your subset. \n",
    "\n",
    "```python\n",
    "# Link the DataFrames\n",
    "full_census = census_A.append(census_B_new)\n",
    "```\n",
    "Now that you have your non duplicates, all you need is a simple append using the DataFrame append method of census A, and you have your linked Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471b18e-dfc2-48f8-8d8a-e596a90fb549",
   "metadata": {},
   "source": [
    "To recap, what we did was build on top of our previous work in generating pairs, comparing across columns and finding potential matches. \n",
    "\n",
    "We then isolated all possible matches, where there are matches across 3 columns or more, ensuring we tightened our search for duplicates across both DataFrames before we link them. \n",
    "\n",
    "Extracted the row indices of `census_B` where there are duplicates. Found rows of `census_B` where they are not duplicated with `census_A` by using the tilde symbol. And linked both DataFrames for full census results.\n",
    "```python\n",
    "# Import recordlinkage and generate pairs and compare across columns\n",
    ". . .\n",
    "# Generate potential matches\n",
    "potential_matches = compare_cl.compute(full_pairs, census_A, census_B)\n",
    "\n",
    "# Isolate matches with matching values for 3 or more columns\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get index for matching census_B rows only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "\n",
    "# Finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "\n",
    "# Link the DataFrame\n",
    "full_census = census_A.append(census_B_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e56bb-e6d9-4db6-97c1-178d59c6c217",
   "metadata": {},
   "source": [
    "## Getting the right index\n",
    "Here's a DataFrame named `matches` containing potential matches between two DataFrames, `users_1` and `users_2`. Each DataFrame's row indices is stored in `uid_1` and `uid_2` respectively.\n",
    "```\n",
    "             first_name  address_1  address_2  marriage_status  date_of_birth\n",
    "uid_1 uid_2                                                                  \n",
    "0     3              1          1          1                1              0\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "1     3              1          1          1                1              0\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "```\n",
    "How do you extract all values of the `uid_1` index column?\n",
    "\n",
    "1. **`matches.index.get_level_values(0)`**\n",
    "2. ~`matches.index.get_level_values(1)`~\n",
    "3. **`matches.index.get_level_values('uid_1')`**\n",
    "\n",
    "**Answer: Both 1 and 3 are correct.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd129c2-fbc5-44ab-80c7-dba97853a17b",
   "metadata": {},
   "source": [
    "## Linking them together!\n",
    "In the last lesson, you've finished the bulk of the work on your effort to link `restaurants` and `restaurants_new`. You've generated the different pairs of potentially matching rows, searched for exact matches between the `cuisine_type` and `city` columns, but compared for similar strings in the `rest_name` column. You stored the DataFrame containing the scores in `potential_matches`.\n",
    "\n",
    "Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of `restaurants_new` that are matching across the columns mentioned above from `potential_matches`. Then you will subset `restaurants_new` on these indices, then append the non-duplicate values to restaurants.\n",
    "\n",
    "- Isolate instances of `potential_matches` where the row sum is above or equal to 3 by using the `.sum()` method.\n",
    "- Extract the second column index from `matches`, which represents row indices of matching record from `restaurants_new` by using the `.get_level_values()` method.\n",
    "- Subset `restaurants_new` for rows that are not in `matching_indices`.\n",
    "- Append `non_dup` to `restaurants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527469b-c01e-4ca4-8585-c46180ac4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c67dd-b5d6-4a2f-bb35-3904e6d17865",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "                        rest_name                  rest_addr               city         phone cuisine_type \n",
    "    0   arnie morton's of chicago   435 s. la cienega blv .         los angeles    3102461501     american \n",
    "    1          art's delicatessen       12224 ventura blvd.         studio city    8187621221     american   \n",
    "    2                   campanile       624 s. la brea ave.         los angeles    2139381447     american\n",
    "    3                       fenix    8358 sunset blvd. west           hollywood    2138486677     american\n",
    "    4          grill on the alley           9560 dayton way         los angeles    3102760615     american\n",
    "    ..                        ...                        ...                ...           ...          ...\n",
    "    76                        don        1136 westwood blvd.           westwood    3102091422      italian \n",
    "    77                      feast        1949 westwood blvd.            west la    3104750400      chinese \n",
    "    78                   mulberry        17040 ventura blvd.             encino    8189068881        pizza   \n",
    "    80                    jiraffe      502 santa monica blvd       santa monica    3109176671  californian  \n",
    "    81                   martha's  22nd street grill 25 22nd  st. hermosa beach    3103767786     american  \n",
    "   \n",
    "    [396 rows x 5 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5e98b-b879-4a2f-a794-3a1941a0f6d6",
   "metadata": {},
   "source": [
    "*Linking the DataFrames is arguably the most straightforward step of record linkage. You are now ready to get started on that recommendation engine.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9b8d2-4671-4c05-829c-74f30bfd4c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c6c04-c93c-44fc-b8ef-15879587c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "769e3abd-cf76-4d4d-8b50-7d2bb5cc7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = \"123-456-789\"\n",
    "# Is x a valid phone number?\n",
    "print(bool(re.compile(\"\\d{3}-\\d{3}-\\d{4}\").match(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
