{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59721182-81bc-446b-8985-fec668764506",
   "metadata": {},
   "source": [
    "# 3. Advanced data problems\n",
    "**In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef844234-6c11-4d7d-9197-a258312b19df",
   "metadata": {},
   "source": [
    "## Uniformity\n",
    "We're going to tackle a problem that could similarly skew our data, which is unit uniformity. For example, we can have temperature data that has values in both Fahrenheit and Celsius, weight data in Kilograms and in pounds, dates in multiple formats, and so on. Verifying unit uniformity is imperative to having accurate analysis.\n",
    "\n",
    "Column | Unit\n",
    ":---|:---\n",
    "Temperature | `32°C` **is also** `89.6°F`\n",
    "Weight | `70 kg` **is also** `154 lb`\n",
    "Date | `26-11-2020` **is also** `26, November, 2020`\n",
    "Money | `100$` **is also** `85€`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3791034-b68f-457d-a704-d235251f50fc",
   "metadata": {},
   "source": [
    "### An example\n",
    "Here's a dataset with average temperature data throughout the month of March in New York City. The dataset was collected from different sources with temperature data in Celsius and Fahrenheit merged together. \n",
    "\n",
    "```python\n",
    "temperatures = pd.read_csv('temperatures.csv')\n",
    "temperatures.head()\n",
    "```\n",
    "\n",
    "```\n",
    "       Date  Temperature\n",
    "0  03.03.19         14.0\n",
    "1  04.03.19         15.0\n",
    "2  03.03.19         18.0\n",
    "3  04.03.19         16.0\n",
    "4  03.03.19         62.6  <--\n",
    "```\n",
    "We can see that unless a major climate event occurred, the last value here is most likely Fahrenheit, not Celsius. \n",
    " \n",
    "To confirm the presence of these values visually, we can do so by plotting a scatter plot of our data. We can do this using matplotlib.pyplot, which was imported as plt. We use the plt dot scatter function, which takes in what to plot on the x axis, the y axis, and which data source to use. \n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x = 'date', y = 'temperature', data = temperatures)\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725961bc-11c1-4ea0-b8cd-7dc9212bc64b",
   "metadata": {},
   "source": [
    "Here is the formula for converting Fahrenheit to Celsius.\n",
    "\n",
    "$$\n",
    "C = (F - 32) \\times \\frac{5}{9}\n",
    "$$\n",
    "\n",
    "To convert our temperature data, we isolate all rows of temperature column where it is above 40 using the loc method. We chose 40 because it's a common sense maximum for Celsius temperatures in New York City. \n",
    " \n",
    "```python\n",
    "temp_fah = temperatures.loc[temperatures['Temperature'] > 40, 'Temperatures']\n",
    "temp_cels = (temp_fah - 32) * (5/9)\n",
    "temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n",
    "```\n",
    " \n",
    "We then convert these values to Celsius using the formula above, and reassign them to their respective Fahrenheit values in temperatures. \n",
    "\n",
    "We can make sure that our conversion was correct with an `assert` statement, by making sure the maximum value of temperature is less than 40.\n",
    "\n",
    "```python\n",
    "# Assert conversion is correct\n",
    "assert temperatures['Temperature'].max() < 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c9b14-2a10-4bf8-a03f-cb467fc6ba5c",
   "metadata": {},
   "source": [
    "### Treating date data\n",
    "Here's another common uniformity problem with date data. This is a DataFrame called birthdays containing birth dates for a variety of individuals. It has been collected from a variety of sources and merged into one.\n",
    "\n",
    "```python\n",
    "birthdays.head()\n",
    "```\n",
    "\n",
    "```\n",
    "          Birthday First name Last name\n",
    "0         27/27/19      Rowan     Nunez    <--- ??\n",
    "1         03-29-19      Brynn       Lee    <--- MM-DD-YY\n",
    "2  March 3rd, 2019     Sophia    Reilly    <--- Month Day, YYY\n",
    "3         24-03-19     Deacon    Prince\n",
    "4         06-03-19   Griffith      Neal\n",
    "```\n",
    "The second one has the month, day, year format, whereas the third one has the month written out. The first one is obviously an error, with what looks like a day day year format. \n",
    "\n",
    "### Datetime formatting\n",
    "`datetime` accepts different formats that help you format your dates as pleased.\n",
    "\n",
    "Date | `datetime` format\n",
    ":---|:---\n",
    "25-12-2019 | `%d-%m-%Y`\n",
    "December 25th 2019 | `%c`\n",
    "12-25-2019 | `%m-%d-%Y`\n",
    "... | ...\n",
    "\n",
    "The pandas to datetime function (`pandas.to_datetime()`) automatically accepts most date formats, but could raise errors when certain formats are unrecognizable.\n",
    "\n",
    "### Treating date data\n",
    "\n",
    "You can treat these date inconsistencies easily by converting your date column to datetime. We can do this in pandas with the `to_datetime` function.\n",
    "\n",
    "```python\n",
    "# Converts to datetime\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'])\n",
    "```\n",
    "\n",
    "```\n",
    "ValueError: month must be in 1..12\n",
    "```\n",
    "\n",
    "However this isn't enough and will most likely return an error, since we have dates in multiple formats, especially the weird day/day/format which triggers an error with months. \n",
    "\n",
    "Instead we set the `infer_datetime_format` argument to `True`, and set `errors='coerce'`. This will infer the format and return missing value for dates that couldn't be identified and converted instead of a value error.\n",
    "```python\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'], \n",
    "                                       # Attempt to infer format of each date\n",
    "                                       infer_datetime_format=True,\n",
    "                                       # Return NA for rows where conversion failed\n",
    "                                       errors = 'coerce')\n",
    "```\n",
    "\n",
    "This returns the birthday column with aligned formats, with the initial ambiguous format of day day year, being set to NAT, which represents missing values in Pandas for datetime objects.\n",
    "\n",
    "```python\n",
    "birthdays.head()\n",
    "```\n",
    "\n",
    "```\n",
    "          Birthday First name Last name\n",
    "0              NaT      Rowan     Nunez \n",
    "1       2019-03-29      Brynn       Lee \n",
    "2       2019-03-03     Sophia    Reilly \n",
    "3       2019-03-24     Deacon    Prince\n",
    "4       2019-06-03   Griffith      Neal\n",
    "```\n",
    "\n",
    "### Datetime formatting\n",
    "We can also convert the format of a datetime column using the `dt.strftime` method, which accepts a datetime format of your choice. For example, here we convert the Birthday column to day month year, instead of year month day.\n",
    "\n",
    "```python\n",
    "birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(%d-%m-%Y)\n",
    "birthdays.head()\n",
    "```\n",
    "```\n",
    "          Birthday First name Last name\n",
    "0              NaT      Rowan     Nunez \n",
    "1       29-03-2019      Brynn       Lee \n",
    "2       03-03-2019     Sophia    Reilly \n",
    "3       24-03-2019     Deacon    Prince\n",
    "4       03-06-2019   Griffith      Neal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a11bb3-002d-4cb0-bd4e-a6577bc6daec",
   "metadata": {},
   "source": [
    "### Treating ambiguous date data\n",
    "However a common problem is having ambiguous dates with vague formats. \n",
    "\n",
    "For example, is `03-08-2019` in March or August? Unfortunately there's no clear cut way to spot this inconsistency or to treat it.\n",
    "\n",
    "Depending on the size of the dataset and suspected ambiguities, \n",
    "- we can either convert these dates to `NA`s and deal with them accordingly. \n",
    "- Or if you have additional context on the source of your data, you can probably infer the format. \n",
    "- If the majority of subsequent or previous data is of one format, you can probably infer the format as well. \n",
    "\n",
    "All in all, it is essential to properly **understand where your data comes from**, before trying to treat it, as it will make making these decisions much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d0e46-39ba-4ca8-99ad-4645e7f77c9f",
   "metadata": {},
   "source": [
    "## Uniform currencies\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a387569f-4abb-4f96-b530-183831d22a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "banking = pd.read_csv('banking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2d2190-1dfb-4855-b412-c38d6261f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cust_id</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>Age</th>\n",
       "      <th>acct_amount</th>\n",
       "      <th>inv_amount</th>\n",
       "      <th>fund_A</th>\n",
       "      <th>fund_B</th>\n",
       "      <th>fund_C</th>\n",
       "      <th>fund_D</th>\n",
       "      <th>account_opened</th>\n",
       "      <th>last_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>870A9281</td>\n",
       "      <td>1962-06-09</td>\n",
       "      <td>58</td>\n",
       "      <td>63523.31</td>\n",
       "      <td>51295</td>\n",
       "      <td>30105.0</td>\n",
       "      <td>4138.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>15632.0</td>\n",
       "      <td>02-09-18</td>\n",
       "      <td>22-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>166B05B0</td>\n",
       "      <td>1962-12-16</td>\n",
       "      <td>58</td>\n",
       "      <td>38175.46</td>\n",
       "      <td>15050</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>28-02-19</td>\n",
       "      <td>31-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BFC13E88</td>\n",
       "      <td>1990-09-12</td>\n",
       "      <td>34</td>\n",
       "      <td>59863.77</td>\n",
       "      <td>24567</td>\n",
       "      <td>10323.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>25-04-18</td>\n",
       "      <td>02-04-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F2158F66</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>35</td>\n",
       "      <td>84132.10</td>\n",
       "      <td>23712</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>07-11-17</td>\n",
       "      <td>08-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7A73F334</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>30</td>\n",
       "      <td>120512.00</td>\n",
       "      <td>93230</td>\n",
       "      <td>12158.4</td>\n",
       "      <td>51281.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>18383.0</td>\n",
       "      <td>14-05-18</td>\n",
       "      <td>19-07-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   cust_id  birth_date  Age  acct_amount  inv_amount   fund_A  \\\n",
       "0           0  870A9281  1962-06-09   58     63523.31       51295  30105.0   \n",
       "1           1  166B05B0  1962-12-16   58     38175.46       15050   4995.0   \n",
       "2           2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   \n",
       "3           3  F2158F66  1985-11-03   35     84132.10       23712   3908.0   \n",
       "4           4  7A73F334  1990-05-17   30    120512.00       93230  12158.4   \n",
       "\n",
       "    fund_B   fund_C   fund_D account_opened last_transaction  \n",
       "0   4138.0   1420.0  15632.0       02-09-18         22-02-19  \n",
       "1    938.0   6696.0   2421.0       28-02-19         31-10-18  \n",
       "2   4590.0   8469.0   1185.0       25-04-18         02-04-18  \n",
       "3    492.0   6482.0  12830.0       07-11-17         08-11-18  \n",
       "4  51281.0  13434.0  18383.0       14-05-18         19-07-18  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9d57-3755-4d62-b529-83c30b1ac279",
   "metadata": {},
   "source": [
    "The dataset contains data on the amount of money stored in accounts (`acct_amount`), their currency (`acct_cur`), amount invested (`inv_amount`), account opening date (`account_opened`), and last transaction date (`last_transaction`) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfe9e8-211c-4f10-9512-edca8438f5e8",
   "metadata": {},
   "source": [
    "- Find the rows of `acct_cur` in `banking` that are equal to `'euro'` and store them in the variable `acct_eu`.\n",
    "- Find all the rows of `acct_amount` in `banking` that fit the `acct_eu` condition, and convert them to USD by multiplying them with `1.1`.\n",
    "- Find all the rows of `acct_cur` in `banking` that fit the `acct_eu` condition, set them to `'dollar'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63c79b-d4cf-44d6-a152-49e23c8fe6ae",
   "metadata": {},
   "source": [
    "```python\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452676f9-9b02-43cc-a922-445a5bc6c6f6",
   "metadata": {},
   "source": [
    "## Uniform dates\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The `account_opened` column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a `datetime` object, while making sure that the format is inferred and potentially incorrect formats are set to missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9914c-01f6-4399-b67c-e4b78acf4aa4",
   "metadata": {},
   "source": [
    "- Print the header of `account_opened` from the `banking` DataFrame and take a look at the different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7f3484-9ed6-4985-a245-58d25724ad44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    02-09-18\n",
       "1    28-02-19\n",
       "2    25-04-18\n",
       "3    07-11-17\n",
       "4    14-05-18\n",
       "Name: account_opened, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the header of account_opened\n",
    "banking['account_opened'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31cde0-fc8d-42b7-976f-ff419a53681f",
   "metadata": {},
   "source": [
    "### Question\n",
    "Take a look at the output. You tried converting the values to `datetime` using the default `to_datetime()` function without changing any argument, however received the following error:\n",
    "\n",
    "`ValueError: month must be in 1..12`\n",
    "Why do you think that is?\n",
    "\n",
    "**Answers**\n",
    "\n",
    "1. ~The `to_datetime()` function needs to be explicitly told which date format each row is in.~\n",
    "\n",
    "2. ~The `to_datetime()` function can only be applied on `YY-mm-dd` date formats.~\n",
    "\n",
    "3. **The `21-14-17` entry is erroneous and leads to an error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcda54-bab1-4a83-94d7-c3167627049b",
   "metadata": {},
   "source": [
    "- Convert the `account_opened` column to `datetime`, while making sure the date format is inferred and that erroneous formats that raise error return a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6247ad7f-b22e-4188-b223-7896e36abe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    02-09-18\n",
      "1    28-02-19\n",
      "2    25-04-18\n",
      "3    07-11-17\n",
      "4    14-05-18\n",
      "Name: account_opened, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format=True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba2b01-7ffe-4ff5-944a-14fc49ae4b82",
   "metadata": {},
   "source": [
    "- Extract the year from the amended `account_opened` column and assign it to the `acct_year` column.\n",
    "- Print the newly created `acct_year` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b920d54-7e51-4ca4-addc-153ae28fbedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2018\n",
       "1    2019\n",
       "2    2018\n",
       "3    2017\n",
       "4    2018\n",
       "Name: acct_year, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "banking['acct_year'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f055a4-b1b9-4ed7-95e5-b9d0449b5b1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557c19a-4579-4f3b-a5da-94059c79d432",
   "metadata": {},
   "source": [
    "## Cross field validation\n",
    "\n",
    "### Motivation\n",
    "It contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight. We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "flights = pd.read_csv('flights.csv')\n",
    "flights.head()\n",
    "```\n",
    "```\n",
    "    flight_number  economy_class  business_class  first_class  total_passengers\n",
    "0           DL140            100              60           40               200\n",
    "1           BA248            130             100           70               300\n",
    "2          MEA124            100              50           50               200\n",
    "3          AFR939            140              70           90               300\n",
    "4          TKA101            130             100           20               250\n",
    "```\n",
    "\n",
    "### Cross field validation\n",
    "*The use of **multiple** fields in a dataset to sanity check data intergrity*\n",
    "\n",
    "For example in the flights dataset, this could be summing economy, business and first class values and making sure they are equal to the total passengers on the plane. \n",
    "\n",
    "```\n",
    "    flight_number  economy_class  business_class  first_class  total_passengers\n",
    "0           DL140            100       +      60      +    40        =      200\n",
    "1           BA248            130       +     100      +    70        =      300\n",
    "2          MEA124            100       +      50      +    50        =      200\n",
    "3          AFR939            140       +      70      +    90        =      300\n",
    "4          TKA101            130       +     100      +    20        =      250\n",
    "```\n",
    "This could be easily done in Pandas, by first subsetting on the columns to sum, then using the sum method with the axis argument set to 1 to indicate row wise summing. \n",
    "\n",
    "We then find instances where the total passengers column is equal to the sum of the classes. And find and filter out instances of inconsistent passenger amounts by subsetting on the equality we created with brackets and the tilde symbol.\n",
    "\n",
    "```python\n",
    "sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis = 1)\n",
    "passenger_equ = sum_classes == flights['total_passengers']\n",
    "# Find and filter out rows with inconsistent passenger totals\n",
    "inconsistent_pass = flights[~passenger_equ]\n",
    "consistent_pass = flights[passenger_equ]\n",
    "```\n",
    "\n",
    "### Cross field validation\n",
    "Here's another example containing user IDs, birthdays and age values for a set of users. \n",
    "\n",
    "```python\n",
    "users.head()\n",
    "```\n",
    "```\n",
    "    user_id  Age    Birthday\n",
    "0     32985   22  1998-03-02\n",
    "1     94387   27  1993-12-04\n",
    "2     34236   42  1978-11-24\n",
    "3     12551   31  1989-01-03\n",
    "4     55212   18  2002-07-02\n",
    "```\n",
    "We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today's date and each birthday.\n",
    "\n",
    "We can do this by first making sure the Birthday column is converted to datetime with the pandas to datetime function. We then create an object storing today's date using the datetime package's date dot today function. We then calculate the difference in years between today's date's year, and the year of each birthday by using the dot dt dot year attribute of the user's Birthday column. We then find instances where the calculated ages are equal to the actual age column in the users DataFrame. We then find and filter out the instances where we have inconsistencies using subsetting with brackets and the tilde symbol on the equality we created.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Convert to datetime and get today's date\n",
    "users['Birthday'] = pd.to_datetime(users['Birthday'])\n",
    "today = dt.date.today()\n",
    "# For each row in the Virthday column, calculate year difference\n",
    "age_manual = today.year - users['Birthday'].dt.year\n",
    "# Find instances where ages match\n",
    "age_equ = age_manual == users['Age']\n",
    "# Find and filter out rows with inconsistent age\n",
    "inconsistent_age = users[~age_equ]\n",
    "consistent_age = users[age_equ]\n",
    "```\n",
    "\n",
    "### What to do when we catch inconsistencies?\n",
    "So what should be the course of action in case we spot inconsistencies with cross-field validation? Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our dataset. We can decide to either drop inconsistent data, set it to missing and impute it, or apply some rules due to domain knowledge. All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from and the different sources feeding into it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f3038-5791-4504-ae8c-e5bc3c816cf8",
   "metadata": {},
   "source": [
    "## Cross field or no cross field?\n",
    "Throughout this course, you've been immersed in a variety of data cleaning problems from range constraints, data type constraints, uniformity and more.\n",
    "\n",
    "In this lesson, you were introduced to cross field validation as a means to sanity check your data and making sure you have strong data integrity.\n",
    "\n",
    "Now, you will map different applicable concepts and techniques to their respective categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5a119-5de3-4741-bbf7-4217f48bd121",
   "metadata": {},
   "source": [
    "Cross field validation | Not cross field validation\n",
    ":---|:---\n",
    "Row wise operations such as `.sum(axix = 1)` | Making sure a `subscription_date` column has no values set in the future\n",
    "Confirming the Age provided by users by cross checking their birthdays | The use of the `.astype()` method\n",
    " | Making sure that a `revenue` column is a numeric column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24c8b-42ca-40d1-b289-24312258f19f",
   "metadata": {},
   "source": [
    "## How's our data integrity?\n",
    "New data has been merged into the `banking` DataFrame that contains details on how investments in the `inv_amount` column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the `age` and `birth_date` columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of `inv_amount` and `age` against the amount invested in different funds and customers' birthdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73828640-c6a9-4988-9a91-1690c3eed355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5697dc8-6ab5-4031-b3a0-60b31857cebb",
   "metadata": {},
   "source": [
    "- Find the rows where the sum of all rows of the `fund_columns` in `banking` are equal to the `inv_amount` column.\n",
    "- Store the values of `banking` with consistent `inv_amount` in `consistent_inv`, and those with inconsistent ones in `inconsistent_inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd8e0ad-8a8b-46a4-8cb3-5fd8c9801a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent investments:  8\n"
     ]
    }
   ],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63e287-1ce6-4a4c-9d11-6b20a21bdc1d",
   "metadata": {},
   "source": [
    "- Store today's date into `today`, and manually calculate customers' ages and store them in `ages_manual`.\n",
    "- Find all rows of `banking` where the `age` column is equal to `ages_manual` and then filter `banking` into `consistent_ages` and `inconsistent_ages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6449c1-be1c-409f-9fbe-31f130e7facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1819736c-f77b-40a9-81b7-984b7bba29d5",
   "metadata": {},
   "source": [
    "```\n",
    "Number of inconsistent ages:  4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b844d4-5bb2-4c5c-9a1e-628731c8f5c8",
   "metadata": {},
   "source": [
    "*There are only 8 and 4 rows affected by inconsistent `inv_amount` and `age` values respectively. In this case, it's best to investigate the underlying data sources before deciding on a course of action.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc3c4-d175-4b82-9aed-61d4a3ba541c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c32e02-aedc-49b1-8c09-774dba6648f1",
   "metadata": {},
   "source": [
    "## Completeness\n",
    "\n",
    "### What is missing data?\n",
    "Missing data is one of the most common and most important data cleaning problems. Essentially, missing data is ***when no data value is stored for a variable in an observation***.\n",
    "\n",
    "Missing data is most commonly represented as `NA` or`NaN`, but can take on arbitrary values like `0` or `.`. \n",
    "\n",
    "It's commonly due to **technical** or **human errors**. Missing data can take many forms, so let's take a look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c790ae-cc57-4c7c-a0ac-1f4b4837c333",
   "metadata": {},
   "source": [
    "### Airquality example\n",
    "It contains temperature and CO2 measurements for different dates.\n",
    "```python\n",
    "import pandas as pd\n",
    "airquality = pd.read_csv('airquality.csv')\n",
    "print(airquality)\n",
    "```\n",
    "```\n",
    "            Date   Temperature   CO2\n",
    "987   20/04/2004          16.8   0.0\n",
    "2119  07/06/2004          18.7   0.8\n",
    "2451  20/06/2004         -40.0   NaN    <---\n",
    "1984  01/06/2004          19.6   1.8\n",
    "8299  19/02/2005          11.2   1.2\n",
    "...      ...              ...    ...     \n",
    "```\n",
    "We can see that the CO2 value in this row is represented as NaN.\n",
    "\n",
    "We can find rows with missing values by using the `.isna` method, which returns `True` for missing values and `False` for complete values across all our rows and columns.\n",
    "```python\n",
    "# Return missing values\n",
    "airquality.isna()\n",
    "```\n",
    "```\n",
    "       Date   Temperature    CO2\n",
    "987   False         False  False\n",
    "2119  False         False  False\n",
    "2451  False         False   True\n",
    "1984  False         False  False\n",
    "8299  False         False  False\n",
    "```\n",
    "\n",
    "We can also chain the `.isna` method with the `.sum` method, which returns a breakdown of missing values per column in our dataframe.\n",
    "\n",
    "```python\n",
    "# Get summary of missingness\n",
    "airquality.isna().sum()\n",
    "```\n",
    "```\n",
    "Date             0\n",
    "Temperature      0\n",
    "CO2            366\n",
    "dtype: int64\n",
    "```\n",
    "We notice that the CO2 column is the only column with missing values - let's find out why and dig further into the nature of this missingness by first visualizing our missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebed402-d1fb-4ed1-addc-53d45652430d",
   "metadata": {},
   "source": [
    "### Missingno\n",
    "***Useful package for visualizing and understanding missing data***\n",
    "\n",
    "The missingno package allows to create useful visualizations of our missing data. We visualize the missingness of the airquality DataFrame with the `msno.matrix` function, and show it with pyplot's show function from matplotlib.\n",
    "\n",
    "```python\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize missingness\n",
    "msno.matrix(airquality)\n",
    "plt.show()\n",
    "```\n",
    "The matrix essentially shows how missing values are distributed across a column. We see that missing CO2 values are randomly scattered throughout the column, but is that really the case? Let's dig deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea95bc-0092-4a96-9a20-2ee83e151c03",
   "metadata": {},
   "source": [
    "### Airquality example\n",
    "We first isolate the rows of airquality with missing CO2 values in one DataFrame, and complete CO2 values in another.\n",
    "```python\n",
    "# Isolate missing and complete values aside\n",
    "missing = airquality[airquality['CO2'].isna()]\n",
    "complete = airquality[~airquality['CO2'].isna()]\n",
    "```\n",
    "\n",
    "Then, let's use the describe method on each of the created DataFrames.\n",
    "\n",
    "```python\n",
    "# Describe complete DataFrame\n",
    "complete.describe()\n",
    "```\n",
    "```\n",
    "       Temperature           CO2\n",
    "count  8991.000000   8991.000000\n",
    "mean     18.317829      1.739584\n",
    "std       8.832116      1.537580\n",
    "min      -1.900000      0.000000\n",
    "...       ...           ...\n",
    "max      44.600000     11.900000\n",
    "```\n",
    "\n",
    "```python\n",
    "# Describe complete DataFrame\n",
    "missing.describe()\n",
    "```\n",
    "```\n",
    "       Temperature   CO2\n",
    "count   366.000000   0.0\n",
    "mean    -39.655738   NaN   <---\n",
    "std       5.988716   NaN\n",
    "min     -49.000000   NaN   <---\n",
    "...       ...        ...\n",
    "max     -30.000000   NaN   <---\n",
    "```\n",
    "\n",
    "We see that for all missing values of CO2, they occur at really low temperatures, with the mean temperature at minus 39 degrees and a minimum and maximum of -49 and -30 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364319b-f5be-492a-ae27-0dfc4dc224bd",
   "metadata": {},
   "source": [
    "Let's confirm this visually with the missngno package.\n",
    "\n",
    "We first sort the DataFrame by the temperature column. Then we input the sorted dataframe to the `.matrix` function from msno. This leaves us with this matrix.\n",
    "\n",
    "```python\n",
    "sorted_airquality = airquality.sort_values(by='Temperature')\n",
    "msno.matrix(sorted_airquality)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43e503-9e78-459e-b774-6c6bee711ce9",
   "metadata": {},
   "source": [
    "### Missingness types\n",
    "\n",
    "*Missing Completely at Random*<br/>**(MCAR)** | *Missing at Random*<br/>**(MAR)** | *Missing Not at Random*<br/>**(MNAR)**\n",
    ":---|:---|:---\n",
    "No systematic relationship between missing data and other values<br/><br/>Data entry erros when inputting data | Systematic relationship between missing data and other ***observed*** values<br/><br/>Missing ozone data for high temperatures | Systematic relationship between missing data and ***unobserved*** values<br/><br/>Missing temperature values for high temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfae4ae-2f68-4a5d-a46b-2d6f8dde6b28",
   "metadata": {},
   "source": [
    "### How to deal with missing data?\n",
    "**Simple approaches:**\n",
    "1. Drop missing data\n",
    "2. Impute with statistical measures *(mean, median, mode...)*\n",
    "**More complex approaches:**\n",
    "1. Imputing using an algorithmic approach\n",
    "2. Impute with machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09023ad3-6374-431d-bf79-6b5ced567605",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "We'll just explore the simple approaches to dealing with missing data. Let's grab another look at the header of airquality.\n",
    "\n",
    "```python\n",
    "airquality.head()\n",
    "```\n",
    "```\n",
    "         Date   Temperature   CO2\n",
    "0  05/03/2005           8.5   2.5\n",
    "1  23/08/2004          21.8   0.0\n",
    "2  18/02/2005           6.3   1.8 \n",
    "3  08/02/2005         -31.0   NaN\n",
    "4  13/03/2005          19.9   0.1\n",
    "```\n",
    "\n",
    "### Dropping missing values\n",
    "We can drop missing values, by using the `.dropna` method, alongside the subset argument which lets us pick which column's missing values to drop.\n",
    "\n",
    "```python\n",
    "# Drop missing values\n",
    "airquality_dropped = airquality.dropna(subset = ['CO2'])\n",
    "airquality_dropped.head()\n",
    "```\n",
    "```\n",
    "         Date   Temperature   CO2\n",
    "0  05/03/2005           8.5   2.5\n",
    "1  23/08/2004          21.8   0.0\n",
    "2  18/02/2005           6.3   1.8 \n",
    "4  13/03/2005          19.9   0.1\n",
    "5  02/04/2005          17.0   0.8\n",
    "```\n",
    "\n",
    "### Replacing with statistical measures\n",
    "We can also replace the missing values of CO2 with the mean value of CO2, by using the `.fillna` method, which is in this case 1.73.\n",
    "\n",
    "```python\n",
    "co2_mean = airquality['CO2'].mean()\n",
    "airquality_imputed = airquality.fillna({'CO2': co2_mean})\n",
    "airquality_imputed.head()\n",
    "```\n",
    "```\n",
    "         Date   Temperature        CO2\n",
    "0  05/03/2005           8.5   2.500000\n",
    "1  23/08/2004          21.8   0.000000\n",
    "2  18/02/2005           6.3   1.800000\n",
    "3  08/02/2005         -31.0   1.739584\n",
    "4  13/03/2005          19.9   0.100000\n",
    "```\n",
    "Fillna takes in a dictionary with columns as keys, and the imputed value as values. We can even feed custom values into fillna pertaining to our missing data if we have enough domain knowledge about our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849390aa-e321-4664-867a-eff928e308d0",
   "metadata": {},
   "source": [
    "## Is this missing at random?\n",
    "Missingness types can be described as the following:\n",
    "\n",
    "- **Missing Completely at Random**: No systematic relationship between a column's missing values and other or own values.\n",
    "- **Missing at Random**: There is a systematic relationship between a column's missing values and other ***observed*** values.\n",
    "- **Missing not at Random**: There is a systematic relationship between a column's missing values and ***unobserved*** values.\n",
    "\n",
    "You have a DataFrame containing customer satisfaction scores for a service. What type of missingness is the following?\n",
    "   \n",
    "- *A customer `satisfaction_score` column with missing values for highly dissatisfied customers.*\n",
    "\n",
    "1. ~Missing completely at random.~\n",
    "\n",
    "2. ~Missing at random.~\n",
    "\n",
    "3. **Missing not at random.**\n",
    "\n",
    "**Answer: 3.** This is a clear example of missing not at random, where low values of satisfaction_score are missing because of inherently low satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca28791-85c9-4d5a-a9ac-466c7cea74c0",
   "metadata": {},
   "source": [
    "## Missing investors\n",
    "Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\n",
    "\n",
    "You just received a new version of the `banking` DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing `inv_amount` values.\n",
    "\n",
    "You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bf83d-2c6c-4e22-aab9-3ce2214d6d7a",
   "metadata": {},
   "source": [
    "- Print the number of missing values by column in the `banking` DataFrame.\n",
    "- Plot and show the missingness matrix of `banking` with the `msno.matrix()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4d936-04fa-431b-acac-7f33941a992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75779f71-c2b1-49c2-93cb-f186713acf12",
   "metadata": {},
   "source": [
    "```\n",
    "cust_id              0\n",
    "age                  0\n",
    "acct_amount          0\n",
    "inv_amount          13\n",
    "account_opened       0\n",
    "last_transaction     0\n",
    "dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0560b0b-13af-4901-8a14-4cb382bca1f1",
   "metadata": {},
   "source": [
    "- Isolate the values of `banking` missing values of `inv_amount` into `missing_investors` and with non-missing `inv_amount` values into `investors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde77ec-dd2d-4779-be76-c0ab8c50c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbefd1-d3df-46fa-ba63-c0db3bc37b56",
   "metadata": {},
   "source": [
    "### Question\n",
    "Now that you've isolated `banking` into `investors` and `missing_investors`, use the `.describe()` method on both of these DataFrames in the console to understand whether there are structural differences between them. What do you think is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93a7ae-090c-4742-9ef8-551ff24c7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "investors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a72fc7-95a7-46fa-9097-50272c5b34d9",
   "metadata": {},
   "source": [
    "```\n",
    "             age  ...    inv_amount\n",
    "count  84.000000  ...     84.000000\n",
    "mean   43.559524  ...  44717.885476\n",
    "std    10.411244  ...  26031.246094\n",
    "min    26.000000  ...   3216.720000\n",
    "25%    34.000000  ...  22736.037500\n",
    "50%    45.000000  ...  44498.460000\n",
    "75%    53.000000  ...  66176.802500\n",
    "max    59.000000  ...  93552.690000\n",
    "\n",
    "[8 rows x 3 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba88d78-70ed-4ebe-a964-adabad394e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_investors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001edfb-bf27-4ca0-98d2-146b02ca6d56",
   "metadata": {},
   "source": [
    "```\n",
    "             age  ...  inv_amount\n",
    "count  13.000000  ...         0.0\n",
    "mean   21.846154  ...         NaN\n",
    "std     1.519109  ...         NaN\n",
    "min    20.000000  ...         NaN\n",
    "25%    21.000000  ...         NaN\n",
    "50%    21.000000  ...         NaN\n",
    "75%    23.000000  ...         NaN\n",
    "max    25.000000  ...         NaN\n",
    "\n",
    "[8 rows x 3 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a845b6-8eb8-497c-b441-498f63c5451f",
   "metadata": {},
   "source": [
    "1. ~The data is missing completely at random and there are no drivers behind the missingness.~\n",
    "\n",
    "2. **The `inv_amount` is missing only for young customers, since the average age in `missing_investors` is 22 and the maximum age is 25.**\n",
    "\n",
    "3. ~The `inv_amount` is missing only for old customers, since the average age in `missing_investors` is 42 and the maximum age is 59.~\n",
    "\n",
    "**Answer: 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065e432-9dfa-410b-a234-0d369032e19d",
   "metadata": {},
   "source": [
    "- Sort the `banking` DataFrame by the `age` column and plot the missingness matrix of `banking_sorted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5eb352-3308-49d8-900a-42541fbb0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by='age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc95171-49a4-41ea-8924-8f48be09b15c",
   "metadata": {},
   "source": [
    "## Follow the money\n",
    "In this exercise, you're working with another version of the `banking` DataFrame that contains missing values for both the `cust_id` column and the `acct_amount` column.\n",
    "\n",
    "You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing `cust_id` don't really help you, and that on average `acct_amount` is usually 5 times the amount of `inv_amount`.\n",
    "\n",
    "In this exercise, you will drop rows of `banking` with missing `cust_ids`, and impute missing values of `acct_amount` with some domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205387b-b930-46f6-8f55-5e7ee6aa6b09",
   "metadata": {},
   "source": [
    "- Use `.dropna()` to drop missing values of the `cust_id` column in `banking` and store the results in `banking_fullid`.\n",
    "- Use `inv_amount` to compute the estimated account amounts for `banking_fullid` by setting the amounts equal to `inv_amount * 5`, and assign the results to `acct_imp`.\n",
    "- Impute the missing values of `acct_amount` in `banking_fullid` with the newly created `acct_imp` using `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed7c8e-8998-431f-80e5-d457c901af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount'] * 5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6f004-8321-4547-af8d-644c25ff3d5a",
   "metadata": {},
   "source": [
    "```\n",
    "    cust_id             0\n",
    "    acct_amount         0\n",
    "    inv_amount          0\n",
    "    account_opened      0\n",
    "    last_transaction    0\n",
    "    dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa5914-b094-4ec7-a111-05c8a101546b",
   "metadata": {},
   "source": [
    "*As you can see no missing data left, you can definitely bank on getting your analysis right.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
