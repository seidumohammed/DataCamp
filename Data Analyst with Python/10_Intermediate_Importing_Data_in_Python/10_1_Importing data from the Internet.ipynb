{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27b8861-64c8-49f4-9d1f-788d32190a41",
   "metadata": {},
   "source": [
    "# 1. Importing data from the Internet\n",
    "**The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You'll also learn the basics of scraping and parsing web data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ab077-317e-44a0-a12d-12f4ce632e6c",
   "metadata": {},
   "source": [
    "## Importing flat files from the web\n",
    "### Is it possible to import web data?\n",
    "- You can: go to URL and click to download files\n",
    "- BUT: not reproducible, not scalable\n",
    "\n",
    "### The urllib package\n",
    "- Provides interface for fetching data across the web\n",
    "- `urlopen()` - accepts URLs instead of file names\n",
    "\n",
    "### How to automate file download in Python\n",
    "```python\n",
    "from urllib.request import urlretrieve\n",
    "url = 'http://www.website.com/example-link/example_file.csv'\n",
    "urlretrieve(url, 'example_file.csv')\n",
    "```\n",
    "```\n",
    "('example_file.csv', <http.client.HTTPMessage at 0x103cf1128>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f5437-949e-491e-bda0-a17dd0e64630",
   "metadata": {},
   "source": [
    "## Importing flat files from the web: your turn\n",
    "The flat file you will import will be `'winequality-red.csv'` from the University of California, Irvine's [Machine Learning repository](http://archive.ics.uci.edu/ml/index.html). The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.\n",
    "\n",
    "The URL of the file is\n",
    "```\n",
    "'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "```\n",
    "After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626d2e9-d4f5-406f-b2bd-121a285e6054",
   "metadata": {},
   "source": [
    "- Import the function `urlretrieve` from the subpackage `urllib.request`.\n",
    "- Assign the URL of the file to the variable `url`.\n",
    "- Use the function `urlretrieve()` to save the file locally as `'winequality-red.csv'`.\n",
    "- Execute the remaining code to load `'winequality-red.csv'` in a pandas DataFrame and to view its head to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf3062b-d099-4889-b237-95440e7f0dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and view its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb10fd-4dc8-43e5-84a5-5462fda2d647",
   "metadata": {},
   "source": [
    "## Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using `pandas`. In particular, you can use the function `pd.read_csv()` with the URL as the first argument and the separator `sep` as the second argument.\n",
    "\n",
    "The URL of the file, once again, is\n",
    "```\n",
    "'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba23278-6422-4df4-b97d-ce58f3f950dc",
   "metadata": {},
   "source": [
    "- Assign the URL of the file to the variable `url`.\n",
    "- Read file into a DataFrame `df` using `pd.read_csv()`, recalling that the separator in the file is `';'`.\n",
    "- View the head of the DataFrame `df`.\n",
    "- Execute the rest of the code to plot histogram of the first feature in the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5963e748-0fa3-4849-8be7-9e5b25bffe7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "# View the head of the DataFrame\n",
    "display(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.iloc[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf47d9e-b3ac-4ea3-9d05-2d01afbf15d3",
   "metadata": {},
   "source": [
    "## Importing non-flat files from the web\n",
    "Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the `pandas` function `pd.read_csv()`. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use `pd.read_excel()` to import an Excel spreadsheet.\n",
    "\n",
    "The URL of the spreadsheet is\n",
    "```\n",
    "'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "```\n",
    "Your job is to use `pd.read_excel()` to read in all of its sheets, print the sheet names and then print the head of the first sheet *using its name, not its index*.\n",
    "\n",
    "Note that the output of `pd.read_excel()` is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2e853-fd63-4543-91b1-98714f7c3dae",
   "metadata": {},
   "source": [
    "- Assign the URL of the file to the variable `url`.\n",
    "- Read the file in `url` into a dictionary `xls` using `pd.read_excel()` recalling that, in order to import all sheets you need to pass `None` to the argument `sheet_name`.\n",
    "- Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary `xls`.\n",
    "- View the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is `'1700'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49d2c86-26b5-456a-9ecf-1c4c8ccb634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1700', '1900'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>34.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akrotiri and Dhekelia</td>\n",
       "      <td>34.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>41.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>36.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>-14.307000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 country       1700\n",
       "0            Afghanistan  34.565000\n",
       "1  Akrotiri and Dhekelia  34.616667\n",
       "2                Albania  41.312000\n",
       "3                Algeria  36.720000\n",
       "4         American Samoa -14.307000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xls\n",
    "xls = pd.read_excel(url, sheet_name=None)\n",
    "\n",
    "# Print the sheetnames\n",
    "print(xls.keys())\n",
    "\n",
    "# View the head of the first sheet (using its name, NOT its index)\n",
    "display(xls['1700'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003aaf0-c80e-4b32-a215-bbd2b321088d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ac834-55f0-4f42-aa0d-6aace8d671f0",
   "metadata": {},
   "source": [
    "## HTTP requests to import files from the web\n",
    "### URL\n",
    "- Uniform/Universal Resource Locator\n",
    "- References to web resources\n",
    "- Focus: web addresses\n",
    "- Ingredients:\n",
    "    - Protocol identifier - http:\n",
    "    - Resource name - webpage.com\n",
    "- These specify web addresses uniquely\n",
    "\n",
    "### HTTP\n",
    "- HyperText Transfer Protocol\n",
    "- Foundation of data communication for the web\n",
    "- HTTPS - more secure form of HTTP\n",
    "- Going to a website = sending HTTP request\n",
    "    - GET request\n",
    "- `urlretrieve()` performs a GET request\n",
    "- HTML - HyperText Markup Language\n",
    "\n",
    "### GET requests using urllib\n",
    "```python\n",
    "from urllib.request import urlopen, Request\n",
    "url = \"https://www.website.com/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "```\n",
    "\n",
    "### GET requests using requests\n",
    " According to the requests package website. *\"Requests allows you to send organic, grass-fed HTTP/1 dot 1 requests, without the need for manual labor.\"* and the following organizations claim to use requests internally: *\"Her Majesty's Government, Amazon, Google, Twilio, NPR, Obama for America, Twitter, Sony, and Federal U.S. Institutions that prefer to be unnamed.\"*\n",
    "\n",
    "- One of the most downloaded Python packages\n",
    "\n",
    "```python\n",
    "import requests\n",
    "url = \"https://www.website.com/\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393874a-f74b-4107-8be2-52a3f1a7ac4b",
   "metadata": {},
   "source": [
    "## Performing HTTP requests in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from `\"https://campus.datacamp.com/courses/1606/4135?ex=2\".`\n",
    "\n",
    "In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd71a4-93cd-4979-9733-bb89506d3cf9",
   "metadata": {},
   "source": [
    "- Import the functions `urlopen` and `Request` from the subpackage `urllib.request`.\n",
    "- Package the request to the url `\"https://campus.datacamp.com/courses/1606/4135?ex=2\"` using the function `Request()` and assign it to `request`.\n",
    "- Send the request and catch the response in the variable `response` with the function `urlopen()`.\n",
    "- Run the rest of the code to see the datatype of `response` and to close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c72ba8e-b192-4588-a44d-2cfd739bb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Close the response\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6e522-f9bb-46cf-b05e-833a8542c724",
   "metadata": {},
   "source": [
    "## Printing HTTP request results in Python using urllib\n",
    "You have just packaged and sent a GET request to `\"https://campus.datacamp.com/courses/1606/4135?ex=2\"` and then caught the response. You saw that such a response is a `http.client.HTTPResponse` object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could *read* it to extract the HTML and, in fact, such a `http.client.HTTPResponse` object has an associated `read()` method. In this exercise, you'll build on your previous great work to extract the response and print the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4501fb3-a04b-4698-9f35-47075d108354",
   "metadata": {},
   "source": [
    "- Send the request and catch the response in the variable `response` with the function `urlopen()`, as in the previous exercise.\n",
    "- Extract the response using the `read()` method and store the result in the variable `html`.\n",
    "- Print the string `html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c688ecd-21cd-490e-84c7-8212ff3fc3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\"><head><link rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\" href=\"/apple-touch-icon-57x57.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"/apple-touch-icon-114x114.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"/apple-touch-icon-72x72.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"/apple-touch-icon-144x144.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"60x60\" href=\"/apple-touch-icon-60x60.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"/apple-touch-icon-120x120.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"76x76\" href=\"/apple-touch-icon-76x76.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"/apple-touch-icon-152x152.png\"><link rel=\"icon\" type=\"image/png\" href=\"/favicon.ico\"><link rel=\"icon\" type=\"image/png\" href=\"/favicon-196x196.png\" sizes=\"196x196\"><link rel=\"icon\" type=\"image/png\" href=\"/favicon-96x96.png\" sizes=\"96x96\"><link rel=\"icon\" type'\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html[:1000])\n",
    "\n",
    "# Close the response\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b345ea-a0f2-4aa4-8411-d77968439f99",
   "metadata": {},
   "source": [
    "## Performing HTTP requests in Python using requests\n",
    "Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their `\"http://www.datacamp.com/teach/documentation\"` page.\n",
    "\n",
    "Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9bd0e-99dd-4396-bbba-ced4ebc3d049",
   "metadata": {},
   "source": [
    "- Import the package `requests`.\n",
    "- Assign the URL of interest to the variable `url`.\n",
    "- Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.\n",
    "- Use the `text` attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `text`.\n",
    "- Print the HTML of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9448b0a-b41c-41dc-bf63-21968e3aa683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"en-US\">\n",
      "<head>\n",
      "  <meta charset=\"UTF-8\" />\n",
      "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" />\n",
      "  <meta name=\"robots\" content=\"noindex, nofollow\" />\n",
      "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
      "  <title>Just a moment...</title>\n",
      "  <style type=\"text/css\">\n",
      "    html, body {width: 100%; height: 100%; margin: 0; padding: 0;}\n",
      "    body {background-color: #ffffff; color: #000000; font-family:-apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, \"Helvetica Neue\",Arial, sans-serif; font-size: 16px; line-height: 1.7em;-webkit-font-smoothing: antialiased;}\n",
      "    h1 { text-align: center; font-weight:700; margin: 16px 0; font-size: 32px; color:#000000; line-height: 1.25;}\n",
      "    p {font-size: 20px; font-weight: 400; margin: 8px 0;}\n",
      "    p, .attribution, {text-align: center;}\n",
      "    #spinner {margin: 0 auto 30px auto; display: block;}\n",
      "    .attribution {margin-top: 32px;}\n",
      "    @keyframes fader     { 0% {opacity: 0.2;} 50% {opacity: 1.0;} 100% {opacity: 0.2;} }\n",
      "    @-webkit-keyframes fader { 0% {opacity: 0.2;} 50% {opacity: 1.0;} 100% {opacity: 0.2;} }\n",
      "    #cf-bubbles > .bubbles { animation: fader 1.6s infinite;}\n",
      "    #cf-bubbles > .bubbles:nth-child(2) { animation-delay: .2s;}\n",
      "    #cf-bubbles > .bubbles:nth-child(3) { animation-delay: .4s;}\n",
      "    .bubbles { background-color: #f58220; width:20px; height: 20px; margin:2px; border-radius:100%; display:inline-block; }\n",
      "    a { color: #2c7cb0; text-decoration: none; -moz-transition: color 0.15s ease; -o-transition: color 0.15s ease; -webkit-transition: color 0.15s ease; transition: color 0.15s ease; }\n",
      "    a:hover{color: #f4a15d}\n",
      "    .attribution{font-size: 16px; line-height: 1.5;}\n",
      "    .ray_id{display: block; margin-top: 8px;}\n",
      "    #cf-wrapper #challenge-form { padding-top:25px; padding-bottom:25px; }\n",
      "    #cf-hcaptcha-container { text-align:center;}\n",
      "    #cf-hcaptcha-container iframe { display: inline-block;}\n",
      "  </style>\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text[:2035])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb945-af61-4493-bb4a-45bd5acb214d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6687d-6ede-4b4d-9c5d-acdc756d2479",
   "metadata": {},
   "source": [
    "## Scraping the web in Python\n",
    "### HTML\n",
    "- Mix of unstructured and structured data\n",
    "- Structured data:\n",
    "    - Has pre-defined data model, or\n",
    "    - Organized in a defined manner\n",
    "- Unstructured data: neither of these properties\n",
    "\n",
    "### BeautifulSoup\n",
    "- Parse and extract structured data from HTML\n",
    "- Make tag soup beautiful and extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "395491bc-82a7-4758-837b-cf6d08bd862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b09cd-201a-4b2f-845e-2dbe254db8d2",
   "metadata": {},
   "source": [
    "### Prettified Soup\n",
    "```python\n",
    "print(soup.prettify())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47ff30-e9d8-41ee-8e6b-90f3673812be",
   "metadata": {},
   "source": [
    "### Exploring BeautifulSoup\n",
    "- Many methods such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc5dd7d7-377d-4451-b598-ae022d6d955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d68f5718-4f1c-4ac8-9076-14defb4d78f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Beautiful Soup: We called him Tortoise because he taught us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group  | Zine ]\n",
      "\n",
      "Beautiful Soup\n",
      "\n",
      "You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "\n",
      "Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "\n",
      "Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class externalLink\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "Interested? Read more.\n",
      "Getting and giving support\n",
      "\n",
      "\n",
      "\n",
      "  Beautiful Soup for enterprise available via Tidelift\n",
      " \n",
      "\n",
      "\n",
      "If you have questions, send them to the discussion\n",
      "group. If you find a bug, file it on Launchpad. If it's a security vulnerability, report it confidentially through Tidelift.\n",
      "If you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "\n",
      "\n",
      "If Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "Download Beautiful Soup\n",
      "The current release is Beautiful Soup\n",
      "4.9.3 (October 3, 2020). You can install Beautiful Soup 4 with\n",
      "pip install beautifulsoup4.\n",
      "\n",
      "In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "python-bs4 package (for Python 2) or the\n",
      "python3-bs4 package (for Python 3). In Fedora it's\n",
      "available as the python-beautifulsoup4 package.\n",
      "\n",
      "Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the bs4/ directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using 2to3.)\n",
      "\n",
      "Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n",
      "3. Support for Python 2 will be discontinued on or after December 31,\n",
      "2020—one year after the Python 2 sunsetting date.\n",
      "\n",
      "Beautiful Soup 3\n",
      "Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It does not support Python 3 and it will\n",
      "be discontinued on or after December 31, 2020—one year after the\n",
      "Python 2 sunsetting date. If you have any active projects using\n",
      "Beautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\n",
      "your Python 3 conversion.\n",
      "\n",
      "Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "The current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,\n",
      "2019). It's the BeautifulSoup package on pip. It's also\n",
      "available as python-beautifulsoup in Debian and Ubuntu,\n",
      "and as python-BeautifulSoup in Fedora.\n",
      "\n",
      "Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n",
      "\n",
      "Beautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift.\n",
      "Hall of Fame\n",
      "Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "\n",
      "\"Movable\n",
      " Type\", a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "Jiabao Lin's DXY-COVID-19-Crawler\n",
      "uses Beautiful Soup to scrape a Chinese medical site for information\n",
      "about COVID-19, making it easier for researchers to track the spread\n",
      "of the virus. (Source: \"How open source software is fighting COVID-19\")\n",
      "\n",
      "Reddit uses Beautiful Soup to parse\n",
      "a page that's been linked to and find a representative image.\n",
      "\n",
      "Alexander Harrowell uses Beautiful Soup to track the business\n",
      " activities of an arms merchant.\n",
      "\n",
      "The developers of Python itself used Beautiful Soup to migrate the Python\n",
      "bug tracker from Sourceforge to Roundup.\n",
      "\n",
      "The Lawrence Journal-World\n",
      "uses Beautiful Soup to gather\n",
      "statewide election results.\n",
      "\n",
      "The NOAA's Forecast\n",
      "Applications Branch uses Beautiful Soup in TopoGrabber, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "\n",
      "If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or the discussion\n",
      "group.\n",
      "\n",
      "Development\n",
      "Development happens at Launchpad. You can get the source\n",
      "code or file\n",
      "bugs.\n",
      "This document (source) is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Saturday, October 03 2020, 15:42:16 Nowhere Standard Time and last built on Wednesday, July 21 2021, 21:00:01 Nowhere Standard Time.Crummy is © 1996-2021 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b063fd-6d8a-433f-b8a6-d3fa1270a44f",
   "metadata": {},
   "source": [
    "- `find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "377e9eb3-2cf4-497e-80ad-1a2458415d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Download\n",
      "bs4/doc/\n",
      "#HallOfFame\n",
      "enterprise.html\n",
      "https://code.launchpad.net/beautifulsoup\n",
      "https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "zine/\n",
      "bs4/download/\n",
      "http://lxml.de/\n",
      "http://code.google.com/p/html5lib/\n",
      "bs4/doc/\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "https://tidelift.com/security\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\n",
      "zine/\n",
      "None\n",
      "bs4/download/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "download/3.x/BeautifulSoup-3.2.2.tar.gz\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\n",
      "None\n",
      "http://www.nytimes.com/2007/10/25/arts/design/25vide.html\n",
      "https://github.com/BlankerL/DXY-COVID-19-Crawler\n",
      "https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\n",
      "https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\n",
      "http://www.harrowell.org.uk/viktormap.html\n",
      "http://svn.python.org/view/tracker/importer/\n",
      "http://www2.ljworld.com/\n",
      "http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\n",
      "http://esrl.noaa.gov/gsd/fab/\n",
      "http://laps.noaa.gov/topograbber/\n",
      "http://groups.google.com/group/beautifulsoup/\n",
      "https://launchpad.net/beautifulsoup\n",
      "https://code.launchpad.net/beautifulsoup/\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "/source/software/BeautifulSoup/index.bhtml\n",
      "/self/\n",
      "/self/contact.html\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://www.crummy.com/\n",
      "http://www.crummy.com/software/\n",
      "http://www.crummy.com/software/BeautifulSoup/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b820df3-f7ad-4049-a734-e44430d6218e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to *parse*, *prettify* and *extract* information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own [Benevolent Dictator for Life](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life). In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is `url = 'https://www.python.org/~guido/'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dca3a2-c691-4a86-b2e9-4739ed73b78c",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Import the function `BeautifulSoup` from the package `bs4`.\n",
    "- Assign the URL of interest to the variable `url`.\n",
    "- Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.\n",
    "- Use the `text` attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `html_doc`.\n",
    "- Create a BeautifulSoup object `soup` from the resulting HTML using the function `BeautifulSoup()`.\n",
    "- Use the method `prettify()` on `soup` and assign the result to `pretty_soup`.\n",
    "- Print to prettified HTML to your shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce8a8c68-6f92-42f6-9015-d411b2a574bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!-- Built from main -->\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   <a href=\"images/df20000406.jpg\">\n",
      "    Who I Am\n",
      "   </a>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Read\n",
      "my\n",
      "   <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "    \"King's\n",
      "Day Speech\"\n",
      "   </a>\n",
      "   for some inspiration.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am the author of the\n",
      "   <a href=\"http://www.python.org\">\n",
      "    Python\n",
      "   </a>\n",
      "   programming language.  See also my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   and my\n",
      "   <a href=\"Publications.html\">\n",
      "    publications list\n",
      "   </a>\n",
      "   , a\n",
      "   <a href=\"bio.html\">\n",
      "    brief bio\n",
      "   </a>\n",
      "   , assorted\n",
      "   <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "    writings\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "    presentations\n",
      "   </a>\n",
      "   and\n",
      "   <a href=\"interviews.html\">\n",
      "    interviews\n",
      "   </a>\n",
      "   (all about Python), some\n",
      "   <a href=\"pics.html\">\n",
      "    pictures of me\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://neopythonic.blogspot.com\">\n",
      "    my new blog\n",
      "   </a>\n",
      "   , and\n",
      "my\n",
      "   <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "    old\n",
      "blog\n",
      "   </a>\n",
      "   on Artima.com.  I am\n",
      "   <a href=\"https://twitter.com/gvanrossum\">\n",
      "    @gvanrossum\n",
      "   </a>\n",
      "   on Twitter.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am retired, working on personal projects (and maybe a book).\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   .)  I created Python while at CWI.\n",
      "  </p>\n",
      "  <h3>\n",
      "   How to Reach Me\n",
      "  </h3>\n",
      "  <p>\n",
      "   You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "  </p>\n",
      "  <h3>\n",
      "   My Name\n",
      "  </h3>\n",
      "  <p>\n",
      "   My name often poses difficulties for Americans.\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Pronunciation:\n",
      "   </b>\n",
      "   in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "   <a href=\"guido.au\">\n",
      "    sound clip\n",
      "   </a>\n",
      "   .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Spelling:\n",
      "   </b>\n",
      "   my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Alphabetization:\n",
      "   </b>\n",
      "   in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "  </p>\n",
      "  <h3>\n",
      "   More Hyperlinks\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    Here's a collection of\n",
      "    <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "     essays\n",
      "    </a>\n",
      "    relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "   <li>\n",
      "    I own the official\n",
      "    <a href=\"images/license.jpg\">\n",
      "     <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "     Python license.\n",
      "    </a>\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <h3>\n",
      "   The Audio File Formats FAQ\n",
      "  </h3>\n",
      "  <p>\n",
      "   I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "   <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "    http://www.cnpbagwell.com/audio-faq\n",
      "   </a>\n",
      "   .  And here is a link to\n",
      "   <a href=\"http://sox.sourceforge.net/\">\n",
      "    SOX\n",
      "   </a>\n",
      "   , to which I contributed\n",
      "some early code.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <a href=\"images/internetdog.gif\">\n",
      "   \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "  </a>\n",
      "  <hr/>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e1c2c-84fc-4774-b708-191cfd8433d8",
   "metadata": {},
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the text\n",
    "In the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efacb41b-a7bd-48a4-87ca-44b7608e1c5d",
   "metadata": {},
   "source": [
    "- In the sample code, the HTML response object `html_doc` has already been created: your first task is to Soupify it using the function `BeautifulSoup()` and to assign the resulting soup to the variable soup.\n",
    "- Extract the title from the HTML soup `soup` using the attribute `title` and assign the result to `guido_title`.\n",
    "- Print the title of Guido's webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17d7b247-6854-4444-9eca-a197205b94a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(guido_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefae70-2532-4a27-a147-038bbff6f019",
   "metadata": {},
   "source": [
    "- Extract the text from the HTML soup `soup` using the method `get_text()` and assign to `guido_text`.\n",
    "- Print the text from Guido's webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a70caff7-ad59-4246-b4c9-8fc967f6d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\n",
      "\n",
      "\"Gawky and proud of it.\"\n",
      "Who I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.\n",
      "\n",
      "I am retired, working on personal projects (and maybe a book).\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df9411-eba5-43a6-bf46-ddd9e2d004fc",
   "metadata": {},
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method `find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ff574e7-e8e0-4384-b7d5-0dd82a043d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68214a38-5ab6-4b4d-b28c-a9c52c071187",
   "metadata": {},
   "source": [
    "- Use the method `find_all()` to find all hyperlinks in `soup`, remembering that hyperlinks are defined by the HTML tag `<a>` but passed to `find_all()` without angle brackets; store the result in the variable `a_tags`.\n",
    "- The variable `a_tags` is a results set: your job now is to enumerate over it, using a `for` loop and to print the actual URLs of the hyperlinks; to do this, for every element `link` in `a_tags`, you want to `print()` `link.get('href')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0b7141d-37ce-4e14-82ef-be39ff2b8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b8408-99a3-40d6-a251-89a2b420f3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
