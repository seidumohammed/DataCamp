{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be10d3c-ba65-4ec2-95d1-00e580e2719a",
   "metadata": {},
   "source": [
    "# 1. Importing Data from Flat Files\n",
    "**Practice using pandas to get just the data you want from flat files, learn how to wrangle data types and handle errors, and look into some U.S. tax data along the way.**\n",
    "\n",
    "## Introduction to flat files\n",
    "In this course, we'll focus on ingesting data, a fundamental step in any data science project -- after all, you can't analyze what you can't access. Along the way, you'll learn some techniques to clean messy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595bff0-93f5-4cc8-bf84-16409fbb4060",
   "metadata": {},
   "source": [
    "### pandas\n",
    "To do this, we'll use the Python library `pandas`. pandas was originally developed by Wes McKinney in 2008 for financial quantitative analysis, but today it has a large development community and is used in many disciplines. pandas makes it easy to load and manipulate data, and it integrates with loads of analysis and visualization libraries.\n",
    "\n",
    "### Data Frames\n",
    "Central to `pandas` is the **data frame**. Data frames are two-dimensional data structures.\n",
    "\n",
    "This means they have columns, typically labeled with variable names and rows, which also have labels, known as an index in pandas. The default index is the row number, but you can specify a column as the index, and many types of data can be used. While you can create data frames by hand, you'll usually want to load existing data. Pandas handles many data formats, but let's start with a basic one: flat files.\n",
    "\n",
    "### Flat Files\n",
    "Flat files are **simple**, making them popular for storing and sharing data. They can be exported from database management systems and spreadsheet applications, and many online data portals provide flat file downloads. In a flat file, data is stored as **plain text**, with no formatting like colors or bold type. Each line in the file represents **one row**, with column values separated by a chosen character called a **delimiter**. Usually, the delimiter is a comma, and such files are called **CSVs**, or comma-separated values, but other characters can be used. A single pandas function loads all flat files, no matter the delimiter: read CSV.\n",
    "\n",
    "### Loading CSVs\n",
    "Let's import some tax data published as a CSV by the Internal Revenue Service, the U.S. government's tax collection agency. This file has information about household composition and income by ZIP code, making it useful for social and economic analyses. \n",
    "\n",
    "First, we import pandas as pd. \"pd\" is the conventional nickname for pandas. Then, we pass the file path as a string to pd dot read CSV, and assign the resulting data frame to a variable -- \"tax data\" here. Finally, we check the first few rows of the new data frame with `tax_data.head()`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "tax_data = pd.read_csv('us_tax_data_2016.csv')\n",
    "tax_data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84ff38-82ee-4fc5-ae18-f1f8d4959c17",
   "metadata": {},
   "source": [
    "### Loading Other Flat Files\n",
    "But what if that file used a different delimiter, like a tab? Rather than have different functions for every possible delimiter, pandas lets you import any flat file with read CSV and its sep keyword argument. Let's use a tab-separated version of the same tax file to see what this looks like. Again, we import pandas as pd. Then we pass the file path string to read CSV, but this time, we include another argument, sep. `\\t` represents a tab. Last, we check out the data frame with the head method.\n",
    "```python\n",
    "tax_data = pd.read_csv('us_tax_data_2016.tsv', sep='\\t)\n",
    "tax_data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31263cc7-f14d-44b9-b48c-b82e7beaf082",
   "metadata": {},
   "source": [
    "## Get data from CSVs\n",
    "In this exercise, you'll create a data frame from a CSV file. The United States makes available CSV files containing tax data by ZIP or postal code, allowing us to analyze income information in different parts of the country. We'll focus on a subset of the data, `vt_tax_data_2016.csv`, which has select tax statistics by ZIP code in Vermont in 2016.\n",
    "\n",
    "To load the data, you'll need to import the `pandas` library, then read `vt_tax_data_2016.csv` and assign the resulting data frame to a variable. Then we'll have a look at the data.\n",
    "\n",
    "- Import the `pandas` library as `pd`.\n",
    "- Use `read_csv()` to load `vt_tax_data_2016.csv` and assign it to the variable `data`.\n",
    "- View the first few lines of the data frame with the `head()` method. This code has been written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd282fd5-bcec-4295-b581-36c4843585fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
      "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
      "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
      "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
      "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
      "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
      "\n",
      "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
      "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
      "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
      "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
      "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
      "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
      "\n",
      "   A11901  N11902  A11902  \n",
      "0    9734   88260  138337  \n",
      "1   20029   68760  151729  \n",
      "2   24499   34600   90583  \n",
      "3   21573   21300   67045  \n",
      "4   67761   23320  103034  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV and assign it to the variable data\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# View the first few lines of data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4c3f3d-cecc-426d-ae6e-30f42f915dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the total number of tax returns by income group\n",
    "counts = data.groupby(\"agi_stub\").N1.sum()\n",
    "counts.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdab2a5-1dc9-4a45-bf3a-983639ad10a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Modifying flat file imports\n",
    "In this lesson, we'll look at ways to limit the amount of data imported, and how to make that data easier to work with by naming columns.\n",
    "\n",
    "### U.S. Tax Data\n",
    "Checking the U.S. tax data's shape attribute, we see that it has almost 180,000 rows and 147 columns. \n",
    "\n",
    "```python\n",
    "tax_data = pd.read_csv('us_tax_data_2016.csv')\n",
    "\n",
    "print(tax_data.shape)\n",
    "```\n",
    "```\n",
    "(179796, 147)\n",
    "```\n",
    "Unless your computer has very little memory or is running several other programs, pandas can easily handle data this size, but it is more than we need. We have some options to whittle it down.\n",
    "\n",
    "### Limiting Columns\n",
    "The first way is to choose columns to load with the `usecols` keyword argument. Usecols can take a list of either all column names or all column numbers to import. It can even accept a function to select columns, so that you can, say, import all columns starting with the letter N, but this is a more advanced technique. To focus on the number of tax returns by income band and ZIP code, we can set up a list of either the relevant column names or their numbers. We supply the file path as a string to read CSV as usual. This time, we also pass usecols the list of column names or numbers to load. \n",
    "```python\n",
    "col_names = ['STATEFTIPS', 'STATE', 'zipcode', 'agi_stub', 'N1']\n",
    "col_names = [0, 1, 2, 3, 4]\n",
    "# Choose columns to load by name\n",
    "tax_data_v1 = pd.read_csv('us_tax_data_2016.csv', usecols=col_names)\n",
    "# Choose columns to load by number\n",
    "tax_data_v2 = pd.read_csv('us_tax_data_2016.csv', usecols=col_nums)\n",
    "\n",
    "print(tax_data_v1.equals(tax_data_v2))\n",
    "```\n",
    "```\n",
    "True\n",
    "```\n",
    "We can check to make sure the two ways produce the same result, and they do.\n",
    "\n",
    "### Limiting Rows\n",
    "Another option is to reduce the number of rows imported with the `nrows` argument. When reading the file, we pass an integer of the maximum number of rows we want, 1000 here. \n",
    "```python\n",
    "tax_data_first1000 = pd.read_csv('us_tax_data_2016.csv', nrows=1000)\n",
    "print(tax_data_first1000.shape)\n",
    "```\n",
    "```\n",
    "(1000, 147)\n",
    "```\n",
    "\n",
    "Checking the data frame's shape, we see we have exactly the number of rows we asked for.\n",
    "\n",
    "### Limiting Rows\n",
    "`nrows` is especially handy when combined with the `skiprows` argument to process a file in chunks. `skiprows` accepts a list of row numbers to skip, a function to determine whether to skip a row, or the number of rows to skip. Note that pandas automatically makes the first row imported the header, so if you skip the row with column names, you should also specify that `header=none`. \n",
    "\n",
    "Let's get rows 1000 to 1500. Like before, we use `nrows` to specify how many rows we want, but add that skiprows equals 1000. We skipped the header row, so we also specify there isn't one here.\n",
    "```python\n",
    "tax_data_next500 = pd.read_csv('us_tax_data_2016.csv',\n",
    "                              nrow=500,\n",
    "                              skiprows=1000,\n",
    "                              header=None)\n",
    "```\n",
    "\n",
    "### Limiting Rows\n",
    "Checking the head of the data frame, we see there are no column names. \n",
    "```python\n",
    "print(tax_data_next500.head(1))\n",
    "```\n",
    "```\n",
    "  0  1   2     3    4    5  6   ...  136 137  138 139 140 141 142 143\n",
    "0 1 AL 35565    4   270   0 250 ... 1854 262 1978   0   0   0   0  50\n",
    "\n",
    "[1 rows x 147 columns]\n",
    "```\n",
    "Let's fix this.\n",
    "\n",
    "### Assigning Column Names\n",
    "To assign column names when there aren't any, we use another read CSV argument: `names`. As you probably guessed, names takes a list of column names to use. The list **must** include a name for every column in the data -- **if you only want to rename some columns, it should be done after import**.\n",
    "\n",
    "### Assigning Column Names\n",
    "For datasets with fewer columns, you might manually build the names list with the help of a data dictionary. We want to copy a long list of column names from an existing data frame, so we'll pass the first tax subset to the list function to get the column labels. Then we read the CSV, using nrows and skiprows to get the next 500 records, setting header to none, and passing the list of column names to names. \n",
    "```python\n",
    "col_names = list(tax_data_first1000)\n",
    "tax_data_next500 = pd.read_csv('us_tax_data_2016.csv',\n",
    "                               nrow=500,\n",
    "                               skiprows=1000,\n",
    "                               header=None,\n",
    "                               names=col_names)\n",
    "\n",
    "print(tax_data_next500.head(1))\n",
    "```\n",
    "```\n",
    "  STATEFIPS STATE  zipcode  agi_stub ...  N11901 A11901 N11902 A11902\n",
    "0         1    AL    35565         4 ...      50    222    210    794\n",
    "\n",
    "[1 rows x 147 columns]\n",
    "```\n",
    "\n",
    "Now the data frame has column names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017b289-875a-4cf0-aacc-01d4237f6022",
   "metadata": {},
   "source": [
    "## Import a subset of columns\n",
    "The Vermont tax data contains 147 columns describing household composition, income sources, and taxes paid by ZIP code and income group. Most analyses don't need all these columns. In this exercise, you will create a data frame with fewer variables using `read_csv()`s `usecols` argument.\n",
    "\n",
    "Let's focus on household composition to see if there are differences by geography and income level. To do this, we'll need columns on income group, ZIP code, tax return filing status (e.g., single or married), and dependents. The data uses codes for variable names, so the specific columns needed are in the instructions.\n",
    "\n",
    "- Create a list of columns to use: `zipcode`, `agi_stub` (income group), `mars1` (number of single households), `MARS2` (number of households filing as married), and `NUMDEP` (number of dependents).\n",
    "- Create a data frame from `vt_tax_data_2016.csv` that uses only the selected columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0cef82-6197-404b-aa94-e678414169b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          zipcode   mars1  MARS2  NUMDEP\n",
      "agi_stub                                \n",
      "1         1439444  170320  28480   52490\n",
      "2         1439444  104000  37690   64660\n",
      "3         1439444   39160  45390   47330\n",
      "4         1439444   11670  44410   37760\n",
      "5         1439444    7820  67750   60730\n",
      "6         1439444    1210  16340   16300\n"
     ]
    }
   ],
   "source": [
    "# Create list of columns to use\n",
    "cols = ['zipcode', 'agi_stub', 'mars1', 'MARS2', 'NUMDEP']\n",
    "\n",
    "# Create data frame from csv using only selected columns\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "print(data.groupby('agi_stub').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dd811-a2ef-4549-9f09-c8d9d9a32e90",
   "metadata": {},
   "source": [
    "## Import a file in chunks\n",
    "When working with large files, it can be easier to load and process the data in pieces. Let's practice this workflow on the Vermont tax data.\n",
    "\n",
    "The first 500 rows have been loaded as `vt_data_first500`. You'll get the next 500 rows. To do this, you'll employ several keyword arguments: `nrows` and `skiprows` to get the correct records, `header` to tell `pandas` the data does not have column names, and `names` to supply the missing column names. You'll also want to use the `list()` function to get column names from `vt_data_first500` to reuse.\n",
    "\n",
    "- Use `nrows` and `skiprows` to make a data frame, `vt_data_next500`, with the next 500 rows.\n",
    "- Set the `header` argument so that `pandas` knows there is no header row.\n",
    "- Name the columns in `vt_data_next500` by supplying a list of `vt_data_first500`'s columns to the `names` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36225a91-0f69-46df-be1a-6541b0539cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111580</td>\n",
       "      <td>85090</td>\n",
       "      <td>14170</td>\n",
       "      <td>10740</td>\n",
       "      <td>45360</td>\n",
       "      <td>130630</td>\n",
       "      <td>...</td>\n",
       "      <td>53660</td>\n",
       "      <td>50699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10820</td>\n",
       "      <td>9734</td>\n",
       "      <td>88260</td>\n",
       "      <td>138337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82760</td>\n",
       "      <td>51960</td>\n",
       "      <td>18820</td>\n",
       "      <td>11310</td>\n",
       "      <td>35600</td>\n",
       "      <td>132950</td>\n",
       "      <td>...</td>\n",
       "      <td>74340</td>\n",
       "      <td>221146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12820</td>\n",
       "      <td>20029</td>\n",
       "      <td>68760</td>\n",
       "      <td>151729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>46270</td>\n",
       "      <td>19540</td>\n",
       "      <td>22650</td>\n",
       "      <td>3620</td>\n",
       "      <td>24140</td>\n",
       "      <td>91870</td>\n",
       "      <td>...</td>\n",
       "      <td>44860</td>\n",
       "      <td>266097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10810</td>\n",
       "      <td>24499</td>\n",
       "      <td>34600</td>\n",
       "      <td>90583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30070</td>\n",
       "      <td>5830</td>\n",
       "      <td>22190</td>\n",
       "      <td>960</td>\n",
       "      <td>16060</td>\n",
       "      <td>71610</td>\n",
       "      <td>...</td>\n",
       "      <td>29580</td>\n",
       "      <td>264678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7320</td>\n",
       "      <td>21573</td>\n",
       "      <td>21300</td>\n",
       "      <td>67045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>39530</td>\n",
       "      <td>3900</td>\n",
       "      <td>33800</td>\n",
       "      <td>590</td>\n",
       "      <td>22500</td>\n",
       "      <td>103710</td>\n",
       "      <td>...</td>\n",
       "      <td>39170</td>\n",
       "      <td>731963</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>67761</td>\n",
       "      <td>23320</td>\n",
       "      <td>103034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
       "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
       "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
       "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
       "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
       "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
       "\n",
       "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
       "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
       "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
       "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
       "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
       "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
       "\n",
       "   A11901  N11902  A11902  \n",
       "0    9734   88260  138337  \n",
       "1   20029   68760  151729  \n",
       "2   24499   34600   90583  \n",
       "3   21573   21300   67045  \n",
       "4   67761   23320  103034  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vt_data_first500 = pd.read_csv('vt_tax_data_2016.csv', nrows=500)\n",
    "\n",
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows=500,\n",
    "                       \t\t  skiprows=500,\n",
    "                       \t\t  header=None,\n",
    "                       \t\t  names=list(vt_data_first500))\n",
    "\n",
    "# View the Vermont data frames to confirm they're different\n",
    "display(vt_data_first500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fec1b8d-1bae-403e-a778-cab5e4bc2e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>120</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>170</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>76</td>\n",
       "      <td>130</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>142</td>\n",
       "      <td>50</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>2229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>531</td>\n",
       "      <td>30</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub   N1  mars1  MARS2  MARS4  PREP   N2  \\\n",
       "0         50    VT     5356         2  180    120     40      0    90  250   \n",
       "1         50    VT     5356         3   80     50     40      0    40  150   \n",
       "2         50    VT     5356         4   50      0     40      0    40  110   \n",
       "3         50    VT     5356         5   80     20     50      0    60  170   \n",
       "4         50    VT     5356         6    0      0      0      0     0    0   \n",
       "\n",
       "   ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  A11901  \\\n",
       "0  ...     170     497       0       0       0       0      50      76   \n",
       "1  ...      80     460       0       0       0       0      40     142   \n",
       "2  ...      50     471       0       0       0       0       0       0   \n",
       "3  ...      80    2229       0       0       0       0      30     531   \n",
       "4  ...       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   N11902  A11902  \n",
       "0     130     212  \n",
       "1      50     148  \n",
       "2      30      87  \n",
       "3      30     246  \n",
       "4       0       0  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0121bcd-eb37-4776-ab02-e3821c642402",
   "metadata": {},
   "source": [
    "*The techniques are used here can also be employed to explore data before committing to loading all of it, to skip rows without useful data, or to relabel all columns in a dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c5c7a-01dc-4bab-9d64-7f555704e913",
   "metadata": {},
   "source": [
    "---\n",
    "## Handling errors and missing data\n",
    "So far, we've imported flat files with minor tweaks to set column names and manage the amount of data loaded. This is enough if the data is already in great shape. But what if there are issues with the data or the import?\n",
    "\n",
    "### Common Flat File Import Issues\n",
    "Common issues include incorrect column data types, which can hinder analysis, missing values indicated with custom designators, or records that pandas cannot read. Luckily, read CSV offers ways to address these issues during import, reducing the wrangling needed later on.\n",
    "\n",
    "### Specifying Data Types\n",
    "When importing data, `pandas` infers each column's data type. Sometimes, it guesses wrong. Checking data types in the tax data, we see that pandas interpreted ZIP codes as integers. \n",
    "```python\n",
    "print(tax_data.dtypes)\n",
    "```\n",
    "```\n",
    "STATEFIPS      int64\n",
    "STATE         object\n",
    "zipcode        int64\n",
    "agi_stub       int64\n",
    "N1             int64\n",
    "               ...\n",
    "N11902         int64\n",
    "A11902         int64\n",
    "Length: 147, dtype: object\n",
    "```\n",
    "\n",
    "They're more accurately modeled as strings, though -- ZIP codes are not quantities and include meaningful leading zeros.\n",
    "\n",
    "Instead of letting pandas guess, we can set the data type of any or all columns with read CSV's dtype keyword argument. `dtype` takes a dictionary, where each key is a column name and each value is the data type that column should be. Note that non-standard data types, like pandas categories, must be passed in quotations. Here, we specify that the zipcode column should contain strings, leaving pandas to infer the other columns. Printing the new data frame's dtypes, we see that zipcode's `dtype` is \"object\", which is the pandas counterpart to Python strings.\n",
    "```python\n",
    "tax_date = pd.read_csv('us_tax_data_2016.csv', dtype={'zipcode': str})\n",
    "print(tax_data.dtypes)\n",
    "```\n",
    "```\n",
    "STATEFIPS      int64\n",
    "STATE         object\n",
    "zipcode       object\n",
    "agi_stub       int64\n",
    "N1             int64\n",
    "               ...\n",
    "N11902         int64\n",
    "A11902         int64\n",
    "Length: 147, dtype: object\n",
    "```\n",
    "\n",
    "### Customizing Missing Data Values\n",
    "Missing data is another common issue. `pandas` automatically recognizes some values, like “N/A” or “null”, as missing data, enabling the use of handy data-cleaning functions. But sometimes missing values are represented in ways that pandas won't catch, such as with dummy codes. In the tax data, records were sorted so that the first few have the ZIP code 0, which is not a valid code and should be treated as missing.\n",
    "\n",
    "We can tell pandas to consider these missing data with the `na_values` keyword argument. NA values accepts either a single value, a list of values, or a dictionary of columns and values in that column to treat as missing. Let's pass a dictionary specifying that any zeros in zipcode should be treated as missing data. Then we filter the data using the is NA method on the zipcode column to view rows with missing postal codes.\n",
    "```python\n",
    "tax_data = pd.read_csv('us_tax_data_2016.csv',\n",
    "                       na_values={'zipcode': 0})\n",
    "tax_data[tax_data.zipcode.isna()]\n",
    "```\n",
    "\n",
    "### Lines with Errors\n",
    "One last issue you may face are lines that pandas just can't parse. For example, a record could have more values than there are columns, like the second record in this corrupted version of the tax data.\n",
    "\n",
    "By default, trying to load the file results in a long error, and no data is imported.\n",
    "\n",
    "Luckily, we can change this behavior with two arguments, error bad lines and warn bad lines. Both take Boolean, or true/false values. Setting `error_bad_lines=False` makes pandas skip bad lines and load the rest of the data, instead of throwing an error. `warn_bad_lines` tells pandas whether to display messages when unparseable lines are skipped. Let's try importing the corrupted file again, this time with `error_bad_lines=False` and `warn_bad_lines=True`. \n",
    "```python\n",
    "tax_data = pd.read_csv('us_tax_data_2016_corrupt.csv',\n",
    "                       error_bad_lines=False,\n",
    "                       warn_bad_lines=True)\n",
    "```\n",
    "```\n",
    "b'Skipping line 3: expected 147 fields, saw 148\\n'\n",
    "```\n",
    "\n",
    "Success. A word of caution: if lines were skipped, it's worth investigating what was left out to see if there are underlying issues that should be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da986f7c-f2b6-4b45-bdad-a2e4c9e1372f",
   "metadata": {},
   "source": [
    "## Specify data types\n",
    "When loading a flat file, `pandas` infers the best data type for each column. Sometimes its guesses are off, particularly for numbers that represent groups or qualities instead of quantities.\n",
    "\n",
    "Looking at the data dictionary for `vt_tax_data_2016.csv` reveals two such columns. The `agi_stub` column contains numbers that correspond to income categories, and `zipcode` has 5-digit values that should be strings -- treating them as integers means we lose leading 0s, which are meaningful. Let's specify the correct data types with the `dtype` argument.\n",
    "\n",
    "- Load `vt_tax_data_2016.csv` with no arguments and view the data frame's `dtypes` attribute. Note the data types of `zipcode` and `agi_stub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f605526c-4567-4401-974f-8f5d09c24d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATEFIPS     int64\n",
      "STATE        object\n",
      "zipcode       int64\n",
      "agi_stub      int64\n",
      "N1            int64\n",
      "              ...  \n",
      "A85300        int64\n",
      "N11901        int64\n",
      "A11901        int64\n",
      "N11902        int64\n",
      "A11902        int64\n",
      "Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load csv with no additional arguments\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\")\n",
    "\n",
    "# Print the data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10ab8b-0a62-4797-9c04-d4112cb436dc",
   "metadata": {},
   "source": [
    "- Create a dictionary, `data_types`, specifying that `agi_stub` is `\"category\"` data and `zipcode` is string data.\n",
    "- Reload the CSV with the `dtype` argument and the dictionary to set the correct column data types.\n",
    "- View the data frame's `dtypes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e07c5ee4-9849-4bef-9423-f3b1f4fb74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATEFIPS       int64\n",
      "STATE          object\n",
      "zipcode        object\n",
      "agi_stub     category\n",
      "N1              int64\n",
      "               ...   \n",
      "A85300          int64\n",
      "N11901          int64\n",
      "A11901          int64\n",
      "N11902          int64\n",
      "A11902          int64\n",
      "Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub': 'category',\n",
    "\t\t\t  'zipcode': str}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca7845-6b4c-450f-a90b-2a86231e3d1a",
   "metadata": {},
   "source": [
    "*Setting data types at import requires becoming familiar with the data first, but it can save effort later on. A common workflow is to load the data and explore it, then write code that sets data types at import to share with colleagues or other audiences.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fc2ef-ea8c-4881-ba9e-70a57bb03e11",
   "metadata": {},
   "source": [
    "## Set custom NA values\n",
    "Part of data exploration and cleaning consists of checking for missing or NA values and deciding how to account for them. This is easier when missing values are treated as their own data type. and there are `pandas` functions that specifically target such NA values. `pandas` automatically treats some values as missing, but we can pass additional NA indicators with the `na_values` argument. Here, you'll do this to ensure that invalid ZIP codes in the Vermont tax data are coded as NA.\n",
    "- Create a dictionary, `null_values`, specifying that `0`s in the `zipcode` column should be considered NA values.\n",
    "- Load `vt_tax_data_2016.csv`, using the `na_values` argument and the dictionary to make sure invalid ZIP codes are treated as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8415c8f-fadf-4e54-a75a-fdcb1e7bcb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
      "0         50    VT      NaN         1  111580  85090  14170  10740  45360   \n",
      "1         50    VT      NaN         2   82760  51960  18820  11310  35600   \n",
      "2         50    VT      NaN         3   46270  19540  22650   3620  24140   \n",
      "3         50    VT      NaN         4   30070   5830  22190    960  16060   \n",
      "4         50    VT      NaN         5   39530   3900  33800    590  22500   \n",
      "5         50    VT      NaN         6    9620    600   8150      0   7040   \n",
      "\n",
      "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
      "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
      "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
      "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
      "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
      "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
      "5   26430  ...    9600  894432    3350    4939    4990   20428    3900   \n",
      "\n",
      "   A11901  N11902  A11902  \n",
      "0    9734   88260  138337  \n",
      "1   20029   68760  151729  \n",
      "2   24499   34600   90583  \n",
      "3   21573   21300   67045  \n",
      "4   67761   23320  103034  \n",
      "5   93123    2870   39425  \n",
      "\n",
      "[6 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode': 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                   na_values=null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f9c30-cfef-45dc-aef6-19b8565a1083",
   "metadata": {},
   "source": [
    "*Now that NA values are marked as such, it's possible to use NA-specific functions to do things like count missing values, as we did here, or drop records with missing values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb7e19-47fd-455a-ac4c-36adbe94604a",
   "metadata": {},
   "source": [
    "## Skip bad data\n",
    "In this exercise you'll use `read_csv()` parameters to handle files with bad data, like records with more values than columns. By default, trying to import such files triggers a specific error, `pandas.io.common.CParserError`.\n",
    "\n",
    "Some lines in the Vermont tax data here are corrupted. In order to load the good lines, we need to tell `pandas` to skip errors. We also want `pandas` to warn us when it skips a line so we know the scope of data issues.\n",
    "\n",
    "The exercise code will try to read the file. If there is a `pandas.io.common.CParserError`, the code in the `except` block will run.\n",
    "\n",
    "- Try to import the file `vt_tax_data_2016_corrupt.csv` without any keyword arguments.\n",
    "\n",
    "```python\n",
    "try:\n",
    "  # Import the CSV without any keyword arguments\n",
    "  data = pd.read_csv('vt_tax_data_2016_corrupt.csv')\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "```\n",
    "```\n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "Your data contained rows that could not be parsed.\n",
    "```\n",
    "\n",
    "- Import `vt_tax_data_2016_corrupt.csv` with the `error_bad_lines` parameter set to skip bad records.\n",
    "\n",
    "```python\n",
    "try:\n",
    "  # Import CSV with error_bad_lines set to skip bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "```\n",
    "```\n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "   STATEFIPS STATE  zipcode  agi_stub      N1  ...  A85300  N11901  A11901  N11902  A11902\n",
    "0         50    VT        0         1  111580  ...       0   10820    9734   88260  138337\n",
    "1         50    VT        0         2   82760  ...       0   12820   20029   68760  151729\n",
    "2         50    VT        0         3   46270  ...       0   10810   24499   34600   90583\n",
    "3         50    VT        0         5   39530  ...       0   12500   67761   23320  103034\n",
    "4         50    VT        0         6    9620  ...   20428    3900   93123    2870   39425\n",
    "\n",
    "[5 rows x 147 columns]\n",
    "```\n",
    "\n",
    "- Update the import with the `warn_bad_lines` parameter set to issue a warning whenever a bad record is skipped.\n",
    "\n",
    "```python\n",
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines=True)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "```\n",
    "```\n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "Skipping line 5: expected 147 fields, saw 148\n",
    "Skipping line 9: expected 147 fields, saw 148\n",
    "Skipping line 51: expected 147 fields, saw 148\n",
    "\n",
    "   STATEFIPS STATE  zipcode  agi_stub      N1  ...  A85300  N11901  A11901  N11902  A11902\n",
    "0         50    VT        0         1  111580  ...       0   10820    9734   88260  138337\n",
    "1         50    VT        0         2   82760  ...       0   12820   20029   68760  151729\n",
    "2         50    VT        0         3   46270  ...       0   10810   24499   34600   90583\n",
    "3         50    VT        0         5   39530  ...       0   12500   67761   23320  103034\n",
    "4         50    VT        0         6    9620  ...   20428    3900   93123    2870   39425\n",
    "\n",
    "[5 rows x 147 columns]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
